AMQP: Make it easier to disable accepting non-SASL clients and add more tests. Make it easier to turn off support for non-SASL AMQP clients via 'wireFormat.allowNonSaslConnections=false' on the transportConnector URI and add some more tests for this area.  Improvement AMQP: Make it easier to disable accepting non-SASL clients and add more tests. Make it easier to turn off support for non-SASL AMQP clients via 'wireFormat.allowNonSaslConnections=false' on the transportConnector URI and add some more tests for this area. 
AMQP: Static code analysis of activemq-amqp Perform a static code analysis of activemq-amqp and fix some minor issues that it shows. Task AMQP: Static code analysis of activemq-amqp Perform a static code analysis of activemq-amqp and fix some minor issues that it shows.
Container-id field blank on sending an AMQP 1.0 open frame to the client Attaching an AMQP 1.0 receiver client, the container-id field in the open frame is blank. It could be set with the broker id or broker name at least. Improvement Container-id field blank on sending an AMQP 1.0 open frame to the client Attaching an AMQP 1.0 receiver client, the container-id field in the open frame is blank. It could be set with the broker id or broker name at least.
Update commons libraries to latest bugfix release versions Update some commons deps that have later bugfix versions:{noformat}    commons-beanutils-version -> 1.9.2    commons-io-version -> 2.5    commons-logging-version -> 1.2    commons-net-version -> 3.5{noformat} Task Update commons libraries to latest bugfix release versions Update some commons deps that have later bugfix versions:{noformat}    commons-beanutils-version -> 1.9.2    commons-io-version -> 2.5    commons-logging-version -> 1.2    commons-net-version -> 3.5{noformat}
ACK rewrite task does unnecessary syncs When rewriting acks from an older journal to a new one the rewrite task forces a disk sync on each ack written, should instead just queue the write and allow the batch write to sync when it is full or the appender is closed.  Improvement ACK rewrite task does unnecessary syncs When rewriting acks from an older journal to a new one the rewrite task forces a disk sync on each ack written, should instead just queue the write and allow the batch write to sync when it is full or the appender is closed. 
KahaDB does journal recovery for last append in error in normal restart case On a normal restart - the journal is replayed from the last append location in error. Reporting some unnecessary info logging of the form{code}INFO | Recovering from the journal @1:503 INFO | Recovery replayed 1 operations from the journal in 0.0 seconds.{code}Recovery is only required when the last append location is different from the recovery location. Improvement KahaDB does journal recovery for last append in error in normal restart case On a normal restart - the journal is replayed from the last append location in error. Reporting some unnecessary info logging of the form{code}INFO | Recovering from the journal @1:503 INFO | Recovery replayed 1 operations from the journal in 0.0 seconds.{code}Recovery is only required when the last append location is different from the recovery location.
Performance issue in PrioritizedPendingListIterator Sending and consuming 5000 messages to/from a queue, one can see heavy CPU use on the broker side (v 5.12.2).Yourkit shows PrioritizedPendingList$PrioritizedPendingListIterator.<init>as a hot spot method. It calls ArrayList.add(Object) around 12 mio times.Situation is that FilePendingMessageCursor.isEmpty() iterates over in-memory messages and therefore (as it is a prioritized queue) uses PrioritizedPendingListIterator which uses OrderedPendingList.getAsList() which overall turns out to be an expensive method as it converts the self-managed linked list to a Java ArrayList and then this list is filled into another ArrayList managed by PrioritizedPendingListIterator.PrioritizedPendingListIterator could be improved to walk the priority lists via OrderedPendingList iterators, as these are implemented efficiently.  Improvement Performance issue in PrioritizedPendingListIterator Sending and consuming 5000 messages to/from a queue, one can see heavy CPU use on the broker side (v 5.12.2).Yourkit shows PrioritizedPendingList$PrioritizedPendingListIterator.<init>as a hot spot method. It calls ArrayList.add(Object) around 12 mio times.Situation is that FilePendingMessageCursor.isEmpty() iterates over in-memory messages and therefore (as it is a prioritized queue) uses PrioritizedPendingListIterator which uses OrderedPendingList.getAsList() which overall turns out to be an expensive method as it converts the self-managed linked list to a Java ArrayList and then this list is filled into another ArrayList managed by PrioritizedPendingListIterator.PrioritizedPendingListIterator could be improved to walk the priority lists via OrderedPendingList iterators, as these are implemented efficiently. 
Max Frame Size Error exception shows incorrect values at times If the value is below the size of 1 MB the error message reads 0 MB instead of scaling down to bytes or KBs Improvement Max Frame Size Error exception shows incorrect values at times If the value is below the size of 1 MB the error message reads 0 MB instead of scaling down to bytes or KBs
KahaDB: Allow rewrite of message acks in older logs which prevent cleanup There are cases where a chain of journal logs can grow due to acks for messages in older logs needing to be kept so that on recovery proper state can be restored and older messages not be resurrected.  In many cases just moving the acks from one log forward to a new log can free an entire chain during subsequent GC cycles.  The 'compacted' ack log can be written during the time between GC cycles without the index lock being held meaning normal broker operations can continue.   Improvement KahaDB: Allow rewrite of message acks in older logs which prevent cleanup There are cases where a chain of journal logs can grow due to acks for messages in older logs needing to be kept so that on recovery proper state can be restored and older messages not be resurrected.  In many cases just moving the acks from one log forward to a new log can free an entire chain during subsequent GC cycles.  The 'compacted' ack log can be written during the time between GC cycles without the index lock being held meaning normal broker operations can continue.  
Improve nio transport scalability NIO transport uses unbounded thread pool executor to handle read operation. Under large number of connections and load, this could lead to large number of threads and eventually OOM errors. Which is the exact problem that nio transport is supposed to solve. Some work has been done in [AMQ-5480], to make this configurable, but there's still more work to make it more robust. Creating a fixed thread pool with a queue in front gives much better results in my tests.Additionally, the same thread pool is used for accepting connections ([AMQ-5269]). This can lead to the broker not being able to accept new connections under the load. I got much better results when experimenting with implementing acceptor logic directly and handling it in the same thread (without reintroducing the old problem). With these two improvements in place, the broker accept and handle the number of connections up to the system limits. Improvement Improve nio transport scalability NIO transport uses unbounded thread pool executor to handle read operation. Under large number of connections and load, this could lead to large number of threads and eventually OOM errors. Which is the exact problem that nio transport is supposed to solve. Some work has been done in [AMQ-5480], to make this configurable, but there's still more work to make it more robust. Creating a fixed thread pool with a queue in front gives much better results in my tests.Additionally, the same thread pool is used for accepting connections ([AMQ-5269]). This can lead to the broker not being able to accept new connections under the load. I got much better results when experimenting with implementing acceptor logic directly and handling it in the same thread (without reintroducing the old problem). With these two improvements in place, the broker accept and handle the number of connections up to the system limits.
examples/other/perfharness misses readme and uses broken link. The Perfharness demo in examples/other/perfharness * misses a short readme.md that describes the demo* refers to broker link http://www.alphaworks.ibm.com/tech/perfharness in perfharness-activemq.sh Improvement examples/other/perfharness misses readme and uses broken link. The Perfharness demo in examples/other/perfharness * misses a short readme.md that describes the demo* refers to broker link http://www.alphaworks.ibm.com/tech/perfharness in perfharness-activemq.sh
Add option to configure trustAllPackages on Camel ActiveMQ component Sadly we forgot to add it here, so Camel end users have more trouble to set this option.We should make it dead easy as an option. Improvement Add option to configure trustAllPackages on Camel ActiveMQ component Sadly we forgot to add it here, so Camel end users have more trouble to set this option.We should make it dead easy as an option.
queue sendLock prevents concurrent journal updates concurrent producers contend on the queue sendlock rather than on the journal, preventing batching or in the case of jdbc, concurrent writes to the db.The sendlock protects message order, however order is ultimately determined by the sequence id assigned by the persistence adapter. The ground work for aligning the cursors with the journal sequence ids was layed in https://issues.apache.org/jira/browse/AMQ-4485 and https://issues.apache.org/jira/browse/AMQ-5266With the ordering between transacted and non transacted producers overlapping the cursor already respects the journal ordering, the sendLock need no longer protect the journal update.With parallel journal updates we get the benefit of write batching (sharing an fsync) for multiple producers on a single destination. Improvement queue sendLock prevents concurrent journal updates concurrent producers contend on the queue sendlock rather than on the journal, preventing batching or in the case of jdbc, concurrent writes to the db.The sendlock protects message order, however order is ultimately determined by the sequence id assigned by the persistence adapter. The ground work for aligning the cursors with the journal sequence ids was layed in https://issues.apache.org/jira/browse/AMQ-4485 and https://issues.apache.org/jira/browse/AMQ-5266With the ordering between transacted and non transacted producers overlapping the cursor already respects the journal ordering, the sendLock need no longer protect the journal update.With parallel journal updates we get the benefit of write batching (sharing an fsync) for multiple producers on a single destination.
Pre-configure jolokia We should configure jolokia in the broker Improvement Pre-configure jolokia We should configure jolokia in the broker
The corePoolSize value of the TaskRunnerFactory created Executor should be configurable It is currently not possible to alter the corePoolSize value of the Executor created in the TaskRunnerFactory in order to keep some Threads always active and avoid some Thread churn that might otherwise happen.   Improvement The corePoolSize value of the TaskRunnerFactory created Executor should be configurable It is currently not possible to alter the corePoolSize value of the Executor created in the TaskRunnerFactory in order to keep some Threads always active and avoid some Thread churn that might otherwise happen.  
AMQP: Remove deprecated prefetch size configuration option from the transport Remove the previously deprecated setPrefetch option from the AmqpTransportFilter as this value is not used and is set by the client properties.   Task AMQP: Remove deprecated prefetch size configuration option from the transport Remove the previously deprecated setPrefetch option from the AmqpTransportFilter as this value is not used and is set by the client properties.  
Remove deprecated methods from JMX tree Remove the deprecated methods from the JMX tree{code}JobSchedulerViewMBean#removeJobAtScheduledTime{code}Marked as deprecated in 2014 first appeared as deprecated in 5.11.0{code}SubscriptionViewMBean#getSubcriptionIdSubscriptionViewMBean#getSubcriptionName{code}Marked as deprecated in 2013 first appeared as deprecated in 5.10.0 Improvement Remove deprecated methods from JMX tree Remove the deprecated methods from the JMX tree{code}JobSchedulerViewMBean#removeJobAtScheduledTime{code}Marked as deprecated in 2014 first appeared as deprecated in 5.11.0{code}SubscriptionViewMBean#getSubcriptionIdSubscriptionViewMBean#getSubcriptionName{code}Marked as deprecated in 2013 first appeared as deprecated in 5.10.0
AMQP: Update Qpid JMS to the latest release Update to the latest release of Qpid JMS Task AMQP: Update Qpid JMS to the latest release Update to the latest release of Qpid JMS
AMQP: Update to latest bugfix release of Proton-J Update proton to latest version which includes some fixes for credit handling issues. Improvement AMQP: Update to latest bugfix release of Proton-J Update proton to latest version which includes some fixes for credit handling issues.
Remove deprecated getXURL methods from the BrokerServiceMBean The methods getOpenWireURL(), getStompURL(), getSslURL(), and getStompSslURL() were marked deprecated in the 5.7.0 release in AMQ-3918 and should be removed now.  This patchwork of URL accessors don't reflect the full set of transport connectors that can be configured and won't provide correct responses depending on configuration.   Task Remove deprecated getXURL methods from the BrokerServiceMBean The methods getOpenWireURL(), getStompURL(), getSslURL(), and getStompSslURL() were marked deprecated in the 5.7.0 release in AMQ-3918 and should be removed now.  This patchwork of URL accessors don't reflect the full set of transport connectors that can be configured and won't provide correct responses depending on configuration.  
Have an option to error out if a limit is exceeded If a broker exits due to no space {code}   <ioExceptionHandler>	      <defaultIOExceptionHandler ignoreNoSpaceErrors="false"/>	  </ioExceptionHandler>{code} i.e: some other process uses disk space. On a restart it will reset the limits and block pending space. Flow control will kick in. It may not be possible to consume messages in this stage. However it will not allow clients to failover to another broker.I think we should have an option to force the broker to error out, or fail to start if limits are set and they cannot be satisfied.This will ensure that the broker will not accept connections in that state.it could be{code}<broker adjustUsageLimits="false" ..>{code}When false, if the limits are exceeded, store usage or memory usage - the broker fails to start. When true, it adjusts them (like today) to what is available.As ever, naming things is hard. Any better name?  Sub-task Have an option to error out if a limit is exceeded If a broker exits due to no space {code}   <ioExceptionHandler>	      <defaultIOExceptionHandler ignoreNoSpaceErrors="false"/>	  </ioExceptionHandler>{code} i.e: some other process uses disk space. On a restart it will reset the limits and block pending space. Flow control will kick in. It may not be possible to consume messages in this stage. However it will not allow clients to failover to another broker.I think we should have an option to force the broker to error out, or fail to start if limits are set and they cannot be satisfied.This will ensure that the broker will not accept connections in that state.it could be{code}<broker adjustUsageLimits="false" ..>{code}When false, if the limits are exceeded, store usage or memory usage - the broker fails to start. When true, it adjusts them (like today) to what is available.As ever, naming things is hard. Any better name? 
AMQP: Add support for testing transactions with the test client. Update the AMQP test client with the ability to do work in transactions and add some transaction based tests to validate broker behaviour.  Task AMQP: Add support for testing transactions with the test client. Update the AMQP test client with the ability to do work in transactions and add some transaction based tests to validate broker behaviour. 
AMQP: Add support for sending scheduled message using message annotations Add support for reading scheduled message instructions from specific Message Annotations that are mapped into values that work with the built-in broker scheduler feature.||Annotation Name||Description|||x-opt-delivery-time|Analogous to the JMS 2.0 delivery time message property.  Value is set in millisecondssince the Unix Epoch.||x-opt-delivery-delay|Time in Milliseconds to wait before dispatching the message.||x-opt-delivery-repeat|Number of time to reschedule a message sent with a fixed delay.||x-opt-delivery-period|The time in ms to wait between successive repeats of a scheduled message.||x-opt-delivery-cron|A CronTab entry that controls how a message is scheduled.| New Feature AMQP: Add support for sending scheduled message using message annotations Add support for reading scheduled message instructions from specific Message Annotations that are mapped into values that work with the built-in broker scheduler feature.||Annotation Name||Description|||x-opt-delivery-time|Analogous to the JMS 2.0 delivery time message property.  Value is set in millisecondssince the Unix Epoch.||x-opt-delivery-delay|Time in Milliseconds to wait before dispatching the message.||x-opt-delivery-repeat|Number of time to reschedule a message sent with a fixed delay.||x-opt-delivery-period|The time in ms to wait between successive repeats of a scheduled message.||x-opt-delivery-cron|A CronTab entry that controls how a message is scheduled.|
STOMP: Add support for produce and consume from composite destiantions Parse the destination string and build a proper ActiveMQDestination when the incoming SEND or SUBSCRIBE uses a composite style destination:{noformat}/queue/destA,/queue/destB{noformat} Improvement STOMP: Add support for produce and consume from composite destiantions Parse the destination string and build a proper ActiveMQDestination when the incoming SEND or SUBSCRIBE uses a composite style destination:{noformat}/queue/destA,/queue/destB{noformat}
AMQP: Report more meaningful error conditions when an incoming send fails When an AMQP client send a message to the broker we currently send back a generic failure error condition where some more meaningful values can be generated for security or resource allocation errors. Improvement AMQP: Report more meaningful error conditions when an incoming send fails When an AMQP client send a message to the broker we currently send back a generic failure error condition where some more meaningful values can be generated for security or resource allocation errors.
AMQP: Refill sender credit faster to avoid throttling fast producers We should send back credit a bit faster so that a remote sender that is fast doesn't have to pause waiting for more credit. Improvement AMQP: Refill sender credit faster to avoid throttling fast producers We should send back credit a bit faster so that a remote sender that is fast doesn't have to pause waiting for more credit.
Filter queues by name in admin webconsole When the number of queues in the broker gets somewhat large, I found that the queues list in admin webconsole becomes somewhat cumbersome, especially if one wants to monitor some queues.I wrote a simple filter for this page (using only JSP/HTML and get method), enabling to filter the queues by queue name. No impact on the server (all queues are still queried), it just filters out the result. Improvement Filter queues by name in admin webconsole When the number of queues in the broker gets somewhat large, I found that the queues list in admin webconsole becomes somewhat cumbersome, especially if one wants to monitor some queues.I wrote a simple filter for this page (using only JSP/HTML and get method), enabling to filter the queues by queue name. No impact on the server (all queues are still queried), it just filters out the result.
Update HTTP Client and Core versions Update HttpClient and HttpCore Sub-task Update HTTP Client and Core versions Update HttpClient and HttpCore
Add remove(messageId) jmx operation to offline durable subscription Mirroring the queue remove jmx operaton, have a jmx operation to remove a message from an offline durable subscription. Essentially force an ack for that subscription.Usage: browse to find the required messagId string, then invoke remove(messageId) New Feature Add remove(messageId) jmx operation to offline durable subscription Mirroring the queue remove jmx operaton, have a jmx operation to remove a message from an offline durable subscription. Essentially force an ack for that subscription.Usage: browse to find the required messagId string, then invoke remove(messageId)
Improve performance of virtual topic fanout Virtual topics provide a nice alternative to durable subs. Each durable sub is modeled as a separate queue.There are performance implications however, because a message has to be sent to each of the (fanout) queues. For a durable subs, there is a single message in the journal and just index updates for each sub.To improve performance there are three ways to improve the comparison between virtual topics and durable subs.   # avoid the disk sync associated with enqueue   # do parallel enqueues to get the store batching writes   # introduce message references in the journal to reduce the disk ioFor 1, introducing a transaction (either client side or automatically, broker side) ensures there is a single disk sync on commit.For 2, using an executor to do sends in parallel allows the journal to batch as seen in AMQ-5077For 3, the implementation is a lot more involved; for recovery there needs to be a journal write per destination and reading the journal will require two reads because of the indirection. Tracking gc needs to be handled to ensure the referenced entry is maintained. In short this is a lot of work and will only be visible for large (> 8k) messages where the cost of a large v small journal write is noticeable. The messageId dataLocator provides an entry point for this work but considering that 1 & 2 combined can give a 3x improvement I don't think it is worth the effort (and added complexity) at the moment. Improvement Improve performance of virtual topic fanout Virtual topics provide a nice alternative to durable subs. Each durable sub is modeled as a separate queue.There are performance implications however, because a message has to be sent to each of the (fanout) queues. For a durable subs, there is a single message in the journal and just index updates for each sub.To improve performance there are three ways to improve the comparison between virtual topics and durable subs.   # avoid the disk sync associated with enqueue   # do parallel enqueues to get the store batching writes   # introduce message references in the journal to reduce the disk ioFor 1, introducing a transaction (either client side or automatically, broker side) ensures there is a single disk sync on commit.For 2, using an executor to do sends in parallel allows the journal to batch as seen in AMQ-5077For 3, the implementation is a lot more involved; for recovery there needs to be a journal write per destination and reading the journal will require two reads because of the indirection. Tracking gc needs to be handled to ensure the referenced entry is maintained. In short this is a lot of work and will only be visible for large (> 8k) messages where the cost of a large v small journal write is noticeable. The messageId dataLocator provides an entry point for this work but considering that 1 & 2 combined can give a 3x improvement I don't think it is worth the effort (and added complexity) at the moment.
Update the AMQP example to use the new QPid-JMS client. The AMQP Java example needs to get updated to use the new QPid JMS client instead of the legacy version it currently uses. Improvement Update the AMQP example to use the new QPid-JMS client. The AMQP Java example needs to get updated to use the new QPid JMS client instead of the legacy version it currently uses.
Unnecessary stack trace in case of authorization failure The following stack trace is printed when the authorization exception occurs:{code}2015-06-25 08:37:12,697 [ActiveMQ NIO Worker 42] WARN Service - Async error occurred: java.lang.SecurityException: User bla is not authorized to read from: topic://xxx	at org.apache.activemq.security.AuthorizationBroker.addConsumer(AuthorizationBroker.java:155)	at org.apache.activemq.broker.BrokerFilter.addConsumer(BrokerFilter.java:102)	at org.apache.activemq.broker.BrokerFilter.addConsumer(BrokerFilter.java:102)	at org.apache.activemq.broker.MutableBrokerFilter.addConsumer(MutableBrokerFilter.java:107)	at org.apache.activemq.broker.MutableBrokerFilter.addConsumer(MutableBrokerFilter.java:107)	at org.apache.activemq.broker.TransportConnection.processAddConsumer(TransportConnection.java:667)	at org.apache.activemq.command.ConsumerInfo.visit(ConsumerInfo.java:348)	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:334)	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:188)	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:45)	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:300)	at org.apache.activemq.transport.stomp.StompTransportFilter.sendToActiveMQ(StompTransportFilter.java:97)	at org.apache.activemq.transport.stomp.ProtocolConverter.sendToActiveMQ(ProtocolConverter.java:200)	at org.apache.activemq.transport.stomp.ProtocolConverter.onStompSubscribe(ProtocolConverter.java:664)	at org.apache.activemq.transport.stomp.ProtocolConverter.onStompCommand(ProtocolConverter.java:258)	at org.apache.activemq.transport.stomp.StompTransportFilter.onCommand(StompTransportFilter.java:85)	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)	at org.apache.activemq.transport.stomp.StompCodec.processCommand(StompCodec.java:129)	at org.apache.activemq.transport.stomp.StompCodec.parse(StompCodec.java:100)	at org.apache.activemq.transport.stomp.StompNIOTransport.serviceRead(StompNIOTransport.java:112)	at org.apache.activemq.transport.stomp.StompNIOTransport.access$000(StompNIOTransport.java:44)	at org.apache.activemq.transport.stomp.StompNIOTransport$1.onSelect(StompNIOTransport.java:69)	at org.apache.activemq.transport.nio.SelectorSelection.onSelect(SelectorSelection.java:97)	at org.apache.activemq.transport.nio.SelectorWorker$1.run(SelectorWorker.java:119)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745){code}In keeping with logging transport exceptions, it makes sense to only log the stack trace at debug level. Improvement Unnecessary stack trace in case of authorization failure The following stack trace is printed when the authorization exception occurs:{code}2015-06-25 08:37:12,697 [ActiveMQ NIO Worker 42] WARN Service - Async error occurred: java.lang.SecurityException: User bla is not authorized to read from: topic://xxx	at org.apache.activemq.security.AuthorizationBroker.addConsumer(AuthorizationBroker.java:155)	at org.apache.activemq.broker.BrokerFilter.addConsumer(BrokerFilter.java:102)	at org.apache.activemq.broker.BrokerFilter.addConsumer(BrokerFilter.java:102)	at org.apache.activemq.broker.MutableBrokerFilter.addConsumer(MutableBrokerFilter.java:107)	at org.apache.activemq.broker.MutableBrokerFilter.addConsumer(MutableBrokerFilter.java:107)	at org.apache.activemq.broker.TransportConnection.processAddConsumer(TransportConnection.java:667)	at org.apache.activemq.command.ConsumerInfo.visit(ConsumerInfo.java:348)	at org.apache.activemq.broker.TransportConnection.service(TransportConnection.java:334)	at org.apache.activemq.broker.TransportConnection$1.onCommand(TransportConnection.java:188)	at org.apache.activemq.transport.MutexTransport.onCommand(MutexTransport.java:45)	at org.apache.activemq.transport.AbstractInactivityMonitor.onCommand(AbstractInactivityMonitor.java:300)	at org.apache.activemq.transport.stomp.StompTransportFilter.sendToActiveMQ(StompTransportFilter.java:97)	at org.apache.activemq.transport.stomp.ProtocolConverter.sendToActiveMQ(ProtocolConverter.java:200)	at org.apache.activemq.transport.stomp.ProtocolConverter.onStompSubscribe(ProtocolConverter.java:664)	at org.apache.activemq.transport.stomp.ProtocolConverter.onStompCommand(ProtocolConverter.java:258)	at org.apache.activemq.transport.stomp.StompTransportFilter.onCommand(StompTransportFilter.java:85)	at org.apache.activemq.transport.TransportSupport.doConsume(TransportSupport.java:83)	at org.apache.activemq.transport.stomp.StompCodec.processCommand(StompCodec.java:129)	at org.apache.activemq.transport.stomp.StompCodec.parse(StompCodec.java:100)	at org.apache.activemq.transport.stomp.StompNIOTransport.serviceRead(StompNIOTransport.java:112)	at org.apache.activemq.transport.stomp.StompNIOTransport.access$000(StompNIOTransport.java:44)	at org.apache.activemq.transport.stomp.StompNIOTransport$1.onSelect(StompNIOTransport.java:69)	at org.apache.activemq.transport.nio.SelectorSelection.onSelect(SelectorSelection.java:97)	at org.apache.activemq.transport.nio.SelectorWorker$1.run(SelectorWorker.java:119)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.lang.Thread.run(Thread.java:745){code}In keeping with logging transport exceptions, it makes sense to only log the stack trace at debug level.
improve performance of TextFileCertificateLoginModule when many entries are in the "textfiledn.user " file With a large number of entries ( 200,000 ) in the "org.apache.activemq.jaas.textfiledn.user" file the performance seemed to degrade. To demonstrate the performance difference;{code}1) for 100 entries and calling initialize,login(),commit 10 times - Time taken is 73 miliseconds2) for 200,000 entries and calling initialize,login(),commit 10 times - Time taken is  5020 miliseconds{code}Suggested improvements:- avoid loading the  org.apache.activemq.jaas.textfiledn.user file each time - in PropertiesLoginModule.java, the file is only read when it changes, using the file modification time.- avoid iterating through the Properties object, using a Map instead to retrieve the userName Improvement improve performance of TextFileCertificateLoginModule when many entries are in the "textfiledn.user " file With a large number of entries ( 200,000 ) in the "org.apache.activemq.jaas.textfiledn.user" file the performance seemed to degrade. To demonstrate the performance difference;{code}1) for 100 entries and calling initialize,login(),commit 10 times - Time taken is 73 miliseconds2) for 200,000 entries and calling initialize,login(),commit 10 times - Time taken is  5020 miliseconds{code}Suggested improvements:- avoid loading the  org.apache.activemq.jaas.textfiledn.user file each time - in PropertiesLoginModule.java, the file is only read when it changes, using the file modification time.- avoid iterating through the Properties object, using a Map instead to retrieve the userName
AMQP: Broker should use 'JMS' transformer by default Currently the AMQP transportConnector uses the "NATIVE" transformer by default which limits the interoperability of AMQP client's and client's using other protocols as the messages they'd receive sent from any AMQP connection would be a BytesMessage with the unconverted AMQP message as the payload.By using the JMS transformer as the default we allow AMQP to interoperate just like all the other client connectors attempt to do.  Issues that existed previously with the JMS transformer have been pretty well ironed out now and most all AMQP features work even when using this transformer. Those wishing to gain the small bit of performance that using native transformation would give and who don't require the interop between protocols can configure their transportConnector to use the NATIVE transformer via "transport.transformer=NATIVE" Improvement AMQP: Broker should use 'JMS' transformer by default Currently the AMQP transportConnector uses the "NATIVE" transformer by default which limits the interoperability of AMQP client's and client's using other protocols as the messages they'd receive sent from any AMQP connection would be a BytesMessage with the unconverted AMQP message as the payload.By using the JMS transformer as the default we allow AMQP to interoperate just like all the other client connectors attempt to do.  Issues that existed previously with the JMS transformer have been pretty well ironed out now and most all AMQP features work even when using this transformer. Those wishing to gain the small bit of performance that using native transformation would give and who don't require the interop between protocols can configure their transportConnector to use the NATIVE transformer via "transport.transformer=NATIVE"
AMQP: Support transactions that span multiple session for a single TXN For some clients the ability to have a single TXN that spans multiple sessions is desired so that the TX Coordinator link can be opened in its own session and still allow for links in other sessions to send and receive inside the TX that the coordinator has declared.  At the moment if a client does this it will appear to work in some cases but can leak some memory and any transacted sends from a session not associated with the coordinator link will not be rolled back a redelivered correctly.   New Feature AMQP: Support transactions that span multiple session for a single TXN For some clients the ability to have a single TXN that spans multiple sessions is desired so that the TX Coordinator link can be opened in its own session and still allow for links in other sessions to send and receive inside the TX that the coordinator has declared.  At the moment if a client does this it will appear to work in some cases but can leak some memory and any transacted sends from a session not associated with the coordinator link will not be rolled back a redelivered correctly.  
AMQP: Include broker version information in the Connection properties Similar to our STOMP transport we should return broker version information in the connection properties sent to the remote peer on connect. For new we can use the Symbol values: producer, version and platform  Improvement AMQP: Include broker version information in the Connection properties Similar to our STOMP transport we should return broker version information in the connection properties sent to the remote peer on connect. For new we can use the Symbol values: producer, version and platform 
Add an option to include original message body for advisory messages Currently some advisory messages such as messageConsumed or messageDelivered advisories contain a copy of the original message set as the dataStructure, but the message body is missing because it has been cleared out.This is fine most of the time but it would be nice to be have the ability to optionally include the original message body of messages.  This should be off by default because of the performance hit, but sometimes I want to have the option to receive  the message body content (ie the entire message) on the advisory topic for auditing purposes. New Feature Add an option to include original message body for advisory messages Currently some advisory messages such as messageConsumed or messageDelivered advisories contain a copy of the original message set as the dataStructure, but the message body is missing because it has been cleared out.This is fine most of the time but it would be nice to be have the ability to optionally include the original message body of messages.  This should be off by default because of the performance hit, but sometimes I want to have the option to receive  the message body content (ie the entire message) on the advisory topic for auditing purposes.
Add inflight message size to SubscriptionStatistics It would be useful to know the total message size of messages that are inflight (besides just the count) on a Subscription for monitoring and metrics purposes.  This value can be added to {{SubscriptionStatistics}} and kept in up to date so it can be displayed without blocking.A future enhancement when AMQ-5748 is done could be to add a pending message statistics.  AMQ-5748 is required first to be able to recover message sizes across a broker restart. New Feature Add inflight message size to SubscriptionStatistics It would be useful to know the total message size of messages that are inflight (besides just the count) on a Subscription for monitoring and metrics purposes.  This value can be added to {{SubscriptionStatistics}} and kept in up to date so it can be displayed without blocking.A future enhancement when AMQ-5748 is done could be to add a pending message statistics.  AMQ-5748 is required first to be able to recover message sizes across a broker restart.
AMQP: Return a more complete Source when client looks up an existing durable subscription When a client is looking up an existing durable subscription to resubscribe we need to return a Source instance that contains as much of the original information used to create the durable sub.  Things like noLocal flag and selector used should be returned to the client so that it can validate its request against the subscription that it is attempting to make and fail or otherwise respond if the old one does not match its expectations.  Improvement AMQP: Return a more complete Source when client looks up an existing durable subscription When a client is looking up an existing durable subscription to resubscribe we need to return a Source instance that contains as much of the original information used to create the durable sub.  Things like noLocal flag and selector used should be returned to the client so that it can validate its request against the subscription that it is attempting to make and fail or otherwise respond if the old one does not match its expectations. 
AMQP: Allow delivery transformer to fallback to lower level transformer when transformation fails If a client sends an AMQP that cannot be transformed using the configured transformer the broker shouldn't drop the message, instead it should attempt to fall-back to a less aggressive transformer.  An example would be a broker configured to use the JMS transformer and the incoming message contains a body consisting of a DescribedType.  The JMS Transformer would fail as there is no direct way to map that into a JMS message type.  We could in this case fallback to the Native transformer and still process the message.  An OpenWire client for instance would just receive a BytesMessage while other AMQP clients would get the message in the form it was sent.This allows the message to round-trip for instance from AMQP -> OpenWire -> OpenWire -> AMQP (Broker network bridge) without losing its original payload or message properties.   Improvement AMQP: Allow delivery transformer to fallback to lower level transformer when transformation fails If a client sends an AMQP that cannot be transformed using the configured transformer the broker shouldn't drop the message, instead it should attempt to fall-back to a less aggressive transformer.  An example would be a broker configured to use the JMS transformer and the incoming message contains a body consisting of a DescribedType.  The JMS Transformer would fail as there is no direct way to map that into a JMS message type.  We could in this case fallback to the Native transformer and still process the message.  An OpenWire client for instance would just receive a BytesMessage while other AMQP clients would get the message in the form it was sent.This allows the message to round-trip for instance from AMQP -> OpenWire -> OpenWire -> AMQP (Broker network bridge) without losing its original payload or message properties.  
Cleanup connections that open but don't initiate a protocol handshake. The current inactivity monitor implementation only kicks in when the connection is opened an the protocol handshake completes such as when an OpenWire client sends its WireFormatInfo or when a STOMP client sends the CONNECT frame.We currently deal with this case for AMQP and MQTT by using a connection attempt timeout to close down connections that have opened but not started a handshake.  We should extend this to STOMP and OpenWire as well.  This issue has been seen in the STOMP over websockets where a STOMP WebSocket connection is opened but for some reason never initiates.   Improvement Cleanup connections that open but don't initiate a protocol handshake. The current inactivity monitor implementation only kicks in when the connection is opened an the protocol handshake completes such as when an OpenWire client sends its WireFormatInfo or when a STOMP client sends the CONNECT frame.We currently deal with this case for AMQP and MQTT by using a connection attempt timeout to close down connections that have opened but not started a handshake.  We should extend this to STOMP and OpenWire as well.  This issue has been seen in the STOMP over websockets where a STOMP WebSocket connection is opened but for some reason never initiates.  
Add a SubscriptionStatistics implementation to Subscriptions I think it would be a good idea to create a {{SubscriptionStatistics}} class following the same model as {{DestinationStatistics}}.  There are already several counters/metrics tracked per subscription that could be moved into this new metrics class.  Accessing the values programmatically would benefit because it would it is easier to just call a single gettter and receive a {{SubscriptionStatistics}} object versus having to call several getters to get individual values.A future enhancement after this issue is complete is to expand the metrics and add new ones.  For example, an inflight count is kept but it would also be nice to keep track of the total byte size of the inflight messages as well.  I will be working on a pull request for this over the next couple of days and I will push it up when it is done. New Feature Add a SubscriptionStatistics implementation to Subscriptions I think it would be a good idea to create a {{SubscriptionStatistics}} class following the same model as {{DestinationStatistics}}.  There are already several counters/metrics tracked per subscription that could be moved into this new metrics class.  Accessing the values programmatically would benefit because it would it is easier to just call a single gettter and receive a {{SubscriptionStatistics}} object versus having to call several getters to get individual values.A future enhancement after this issue is complete is to expand the metrics and add new ones.  For example, an inflight count is kept but it would also be nice to keep track of the total byte size of the inflight messages as well.  I will be working on a pull request for this over the next couple of days and I will push it up when it is done.
patch for test: testSelectorsAndNonSelectors() Hello,I have created a patch for TwoBrokerVirtualTopicSelectorAwareForwardingTest.testSelectorsAndNonSelectors() test that was failing in many recent Java 8 builds. (for example https://builds.apache.org/job/ActiveMQ-Java8/org.apache.activemq$activemq-unit-tests/358/#showFailuresLink)The problem was that there was too little time for consumers to consume all messages and therefore the asserts failed. Vladimir Caniga Test patch for test: testSelectorsAndNonSelectors() Hello,I have created a patch for TwoBrokerVirtualTopicSelectorAwareForwardingTest.testSelectorsAndNonSelectors() test that was failing in many recent Java 8 builds. (for example https://builds.apache.org/job/ActiveMQ-Java8/org.apache.activemq$activemq-unit-tests/358/#showFailuresLink)The problem was that there was too little time for consumers to consume all messages and therefore the asserts failed. Vladimir Caniga
MQTT: Allow for configuring maxFrameSize and enforce it across TCP, SSL, NIO and NIO+SSL Allow for transport configuration in MQTT of transport.maxFrameSize to control the max size of incoming mqtt messages.   Sub-task MQTT: Allow for configuring maxFrameSize and enforce it across TCP, SSL, NIO and NIO+SSL Allow for transport configuration in MQTT of transport.maxFrameSize to control the max size of incoming mqtt messages.  
AMQP: Add coverage tests for maxFrameSize handling Implement proper tests to cover the application of the Transport maxFrameSize setting for AMQP and fix any issues that arise during the testing.   Sub-task AMQP: Add coverage tests for maxFrameSize handling Implement proper tests to cover the application of the Transport maxFrameSize setting for AMQP and fix any issues that arise during the testing.  
Implement and test maxFrameSize for STOMP Implement and test {{maxFameSize}} for STOMP to help prevent DOS attacks.  Testing should include TCP, SSL, NIO and NIO+SSL, etc. Sub-task Implement and test maxFrameSize for STOMP Implement and test {{maxFameSize}} for STOMP to help prevent DOS attacks.  Testing should include TCP, SSL, NIO and NIO+SSL, etc.
Implement and test maxFrameSize across all protocols  The {{maxFrameSize}} option that currently exists for the OpenWire protocol should be implemented and tested across all protocols based on the discussion seen here: AMQ-5774.  This will help pevent DOS attacks across any protocol that is used.  Subtasks will be created for the different protocol/transports. New Feature Implement and test maxFrameSize across all protocols  The {{maxFrameSize}} option that currently exists for the OpenWire protocol should be implemented and tested across all protocols based on the discussion seen here: AMQ-5774.  This will help pevent DOS attacks across any protocol that is used.  Subtasks will be created for the different protocol/transports.
Exclude Advisory Topics from expired message processing.   Currently Advisory Topics are included in the Topic checks for expired messages, however advisory Topics will not have messages will TTL so there is no need to schedule the task to periodically check them.   Improvement Exclude Advisory Topics from expired message processing.   Currently Advisory Topics are included in the Topic checks for expired messages, however advisory Topics will not have messages will TTL so there is no need to schedule the task to periodically check them.  
ActiveMQ - Output url to the rest api when starting AMQ We output the url for the web console, but we should also output links to the rest api. Improvement ActiveMQ - Output url to the rest api when starting AMQ We output the url for the web console, but we should also output links to the rest api.
AMQP: Add support for heartbeats and inactivity monitoring. After we update to Proton-J 0.9.1 we will be able to take advantage if the idle processing added in that release to send empty keep alive frames in order to keep idle connections active and detect dropped connections.   New Feature AMQP: Add support for heartbeats and inactivity monitoring. After we update to Proton-J 0.9.1 we will be able to take advantage if the idle processing added in that release to send empty keep alive frames in order to keep idle connections active and detect dropped connections.  
Add some tests for STOMP over WebSockets and fix and improve close handling Add some tests using the Jetty WebSocket client to cover STOMP over websockets and fix some issues with connection close and inactivity handling on the broker side.  We want to ensure that on close or inactivity that we shut down the StompSocket resources.   Improvement Add some tests for STOMP over WebSockets and fix and improve close handling Add some tests using the Jetty WebSocket client to cover STOMP over websockets and fix some issues with connection close and inactivity handling on the broker side.  We want to ensure that on close or inactivity that we shut down the StompSocket resources.  
Configuration to limit the number of Topics/Queues that can be created on broker It would be good to have a configuration with which we can limit the number of topics/queues that can be created in ActiveMQ setup.The will help in cases where some client creates thousands of queues/topics and can potentially affect broker service.For e.g. by default each queue also creates a dispatcher thread so having tens of thousands of queues in one go can potentially cause DOS in broker setup. New Feature Configuration to limit the number of Topics/Queues that can be created on broker It would be good to have a configuration with which we can limit the number of topics/queues that can be created in ActiveMQ setup.The will help in cases where some client creates thousands of queues/topics and can potentially affect broker service.For e.g. by default each queue also creates a dispatcher thread so having tens of thousands of queues in one go can potentially cause DOS in broker setup.
Add exception handler to TaskRunnerFactory The {{TaskRunnerFactory}} class in activemq-client creates a default executor but doesn't set an uncaught exception handler.  This should be added so uncaught errors show up in the log for the created threads.  This is similar to how the {{getExecutor}} method in {{BrokerService}} sets an uncaught exception handler. Improvement Add exception handler to TaskRunnerFactory The {{TaskRunnerFactory}} class in activemq-client creates a default executor but doesn't set an uncaught exception handler.  This should be added so uncaught errors show up in the log for the created threads.  This is similar to how the {{getExecutor}} method in {{BrokerService}} sets an uncaught exception handler.
AMQP: Receiver uses case sensitive string compare to lookup transformer. The AMQP transport is using a case sensitive compare to lookup the requested message transformer so if someone uses 'JMS' instead of 'jms' they get the native variant instead. Improvement AMQP: Receiver uses case sensitive string compare to lookup transformer. The AMQP transport is using a case sensitive compare to lookup the requested message transformer so if someone uses 'JMS' instead of 'jms' they get the native variant instead.
Add clientId and subscriptionName to all durable-related exceptions and log messages within TopicRegion In order to make it easier to debug durable related issues it would be useful if the error messages included the {{clientId}} and {{subscriptionName}}.  The specific use case for this was that a client tried to remove their durable subscription using the wrong {{clientId}} and received an error message which did not include the {{clientId}} so it was difficult to debug.  The error received just said: {{javax.jms.InvalidDestinationException: No durable subscription exists for: <subscriptionName>}}It would be more useful if the error was: {{javax.jms.InvalidDestinationException: No durable subscription exists for clientId: <clientId> and subscriptionName: <subscriptionName>}}I will be attaching a pull request to this ticket with the 2 specific exception messages I would like to see changed in {{TopicRegion}}. Improvement Add clientId and subscriptionName to all durable-related exceptions and log messages within TopicRegion In order to make it easier to debug durable related issues it would be useful if the error messages included the {{clientId}} and {{subscriptionName}}.  The specific use case for this was that a client tried to remove their durable subscription using the wrong {{clientId}} and received an error message which did not include the {{clientId}} so it was difficult to debug.  The error received just said: {{javax.jms.InvalidDestinationException: No durable subscription exists for: <subscriptionName>}}It would be more useful if the error was: {{javax.jms.InvalidDestinationException: No durable subscription exists for clientId: <clientId> and subscriptionName: <subscriptionName>}}I will be attaching a pull request to this ticket with the 2 specific exception messages I would like to see changed in {{TopicRegion}}.
AMQP: Investigate handling of producer flow control and amqp receiver links We need to look into how to handle producer flow control as it affects the broker side of a link (AMQP receiver).  Since we currently are granting credit in batches of 1000 we need to be able to tell that flow control has kicked in and not grant any further credit. Task AMQP: Investigate handling of producer flow control and amqp receiver links We need to look into how to handle producer flow control as it affects the broker side of a link (AMQP receiver).  Since we currently are granting credit in batches of 1000 we need to be able to tell that flow control has kicked in and not grant any further credit.
AMQP: Investigate the affact of abort slow consumer strategy on sender links Need to investigate and write some tests around what happens when the abort slow consumer strategy is in play and it attempts to close the consumer which would be mapped to an AMQP sender end of the link on the broker side.  We probably need to intercept the ConsumerControl and close the appropriate link.   Task AMQP: Investigate the affact of abort slow consumer strategy on sender links Need to investigate and write some tests around what happens when the abort slow consumer strategy is in play and it attempts to close the consumer which would be mapped to an AMQP sender end of the link on the broker side.  We probably need to intercept the ConsumerControl and close the appropriate link.  
AMQP: Update to released Proton-J 0.9.1 Update the Proton-J dependency to 0.9.1 Task AMQP: Update to released Proton-J 0.9.1 Update the Proton-J dependency to 0.9.1
Expose camel component meta model like camel-catalog does Apache Camel has introduced the camel-catalog which enables third party tools to lookup what parameters the component provides together with data types, labels, etc. Basically it generates a json file which contains the needed information and ships it inside the components jar.The ActiveMQ component currently doesn't ship such a json file and it would be good to align with what Camel has done with their components. [~cibsen@e-ma.net] is the right person to ask what exactly is needed.  Improvement Expose camel component meta model like camel-catalog does Apache Camel has introduced the camel-catalog which enables third party tools to lookup what parameters the component provides together with data types, labels, etc. Basically it generates a json file which contains the needed information and ships it inside the components jar.The ActiveMQ component currently doesn't ship such a json file and it would be good to align with what Camel has done with their components. [~cibsen@e-ma.net] is the right person to ask what exactly is needed. 
AMQP: Create unit tests to validate behavior of AMQP implementation against spec and mapping requirements. Now that we can use the simple client for testing we need to create lower level tests to validate AMQP behavior on the broker against the AMQP v1.0 spec and the JMS mapping spec. Sub-task AMQP: Create unit tests to validate behavior of AMQP implementation against spec and mapping requirements. Now that we can use the simple client for testing we need to create lower level tests to validate AMQP behavior on the broker against the AMQP v1.0 spec and the JMS mapping spec.
AMQP: Honor receiver flow as the true prefetch value on a link. We currently don't honor the flow values sent from a receiver to indicate how many messages our sender can produce (prefetch) and always maintain a fixed value based either on a transport configuration option or the first non-zero flow.  This is however not in keeping with how the credit model is expected to work in AMQP and leads to some issues with the brokers sender accepting messages and buffering them until the receiver sends a flow thereby preventing those messages from being dispatched to another receiver subscribed to the same destination.  We need to honor the link credit and only dispatch when the receiver indicates it is willing to take messages by sending more link credit.   Sub-task AMQP: Honor receiver flow as the true prefetch value on a link. We currently don't honor the flow values sent from a receiver to indicate how many messages our sender can produce (prefetch) and always maintain a fixed value based either on a transport configuration option or the first non-zero flow.  This is however not in keeping with how the credit model is expected to work in AMQP and leads to some issues with the brokers sender accepting messages and buffering them until the receiver sends a flow thereby preventing those messages from being dispatched to another receiver subscribed to the same destination.  We need to honor the link credit and only dispatch when the receiver indicates it is willing to take messages by sending more link credit.  
Add safety measure against infinite loop when store exception prevents message removal When the broker is configured with a database store, the "purge" operation enters an infinite loop when the message removal operation fails, for instance when the broker datasource is being restarted (see example stack trace below). Here is a patch which adds a safety measure, in case the "dequeue" count of the queue does not increase between 2 messages removal operations.  The check is not garanteed to detect the problem on the next iteration, because a business consumer might also be dequeuing messages from the queue.  But the "purge" is probably much faster than the business consumer, so if it fails to remove 2 messages in a row, it is enough to detect the problem and abort the infinite loop.{code}2015-03-05 15:38:30,353 | WARN  | 14571659-2202099 |  | JDBCPersistenceAdapter           | Could not get JDBC connection: Data source is closedjava.sql.SQLException: Data source is closed	at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1362)	at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)	at org.apache.activemq.store.jdbc.TransactionContext.getConnection(TransactionContext.java:58)	at org.apache.activemq.store.jdbc.adapter.DefaultJDBCAdapter.getStoreSequenceId(DefaultJDBCAdapter.java:285)	at org.apache.activemq.store.jdbc.JDBCPersistenceAdapter.getStoreSequenceIdForMessageId(JDBCPersistenceAdapter.java:787)	at org.apache.activemq.store.jdbc.JDBCMessageStore.removeMessage(JDBCMessageStore.java:194)	at org.apache.activemq.store.memory.MemoryTransactionStore.removeMessage(MemoryTransactionStore.java:358)	at org.apache.activemq.store.memory.MemoryTransactionStore$1.removeAsyncMessage(MemoryTransactionStore.java:166)	at org.apache.activemq.broker.region.Queue.acknowledge(Queue.java:846)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1602)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1594)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1579)	at org.apache.activemq.broker.region.Queue.purge(Queue.java:1158)	at org.apache.activemq.broker.jmx.QueueView.purge(QueueView.java:54){code} Improvement Add safety measure against infinite loop when store exception prevents message removal When the broker is configured with a database store, the "purge" operation enters an infinite loop when the message removal operation fails, for instance when the broker datasource is being restarted (see example stack trace below). Here is a patch which adds a safety measure, in case the "dequeue" count of the queue does not increase between 2 messages removal operations.  The check is not garanteed to detect the problem on the next iteration, because a business consumer might also be dequeuing messages from the queue.  But the "purge" is probably much faster than the business consumer, so if it fails to remove 2 messages in a row, it is enough to detect the problem and abort the infinite loop.{code}2015-03-05 15:38:30,353 | WARN  | 14571659-2202099 |  | JDBCPersistenceAdapter           | Could not get JDBC connection: Data source is closedjava.sql.SQLException: Data source is closed	at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1362)	at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)	at org.apache.activemq.store.jdbc.TransactionContext.getConnection(TransactionContext.java:58)	at org.apache.activemq.store.jdbc.adapter.DefaultJDBCAdapter.getStoreSequenceId(DefaultJDBCAdapter.java:285)	at org.apache.activemq.store.jdbc.JDBCPersistenceAdapter.getStoreSequenceIdForMessageId(JDBCPersistenceAdapter.java:787)	at org.apache.activemq.store.jdbc.JDBCMessageStore.removeMessage(JDBCMessageStore.java:194)	at org.apache.activemq.store.memory.MemoryTransactionStore.removeMessage(MemoryTransactionStore.java:358)	at org.apache.activemq.store.memory.MemoryTransactionStore$1.removeAsyncMessage(MemoryTransactionStore.java:166)	at org.apache.activemq.broker.region.Queue.acknowledge(Queue.java:846)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1602)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1594)	at org.apache.activemq.broker.region.Queue.removeMessage(Queue.java:1579)	at org.apache.activemq.broker.region.Queue.purge(Queue.java:1158)	at org.apache.activemq.broker.jmx.QueueView.purge(QueueView.java:54){code}
Upgrade to Camel 2.15.0 Upgrade Camel Improvement Upgrade to Camel 2.15.0 Upgrade Camel
AMQP: Add connection property to open frame when connection attempt has failed An AMQP Connection that is opened by a remote peer requires that the broker also send an open frame prior to sending any close frame with an error condition set to indicate failure.  This can lead to the remote peer appearing to be open for a short while before receiving the follow on close frame.  In order to make it know to a client that its open request has actually failed the connection property "amqp:connection-establishment-failed" as a hint that a close frame follows. Sub-task AMQP: Add connection property to open frame when connection attempt has failed An AMQP Connection that is opened by a remote peer requires that the broker also send an open frame prior to sending any close frame with an error condition set to indicate failure.  This can lead to the remote peer appearing to be open for a short while before receiving the follow on close frame.  In order to make it know to a client that its open request has actually failed the connection property "amqp:connection-establishment-failed" as a hint that a close frame follows.
Allow ActiveMQ authentication on server side for REST/Ajax clients. Currently, for REST or Ajax clients to connect to a secured ActiveMQ, the client side needs to send the activemq credentials over HTTP.It should be possible to use a different authentication mechanism for the HTTP connection, and let the servlet use a system account to connect to ActiveMQ.Additionally, it would be great to allow ${} placeholders in the servlet context parameters, which could be resolved by system properties. Improvement Allow ActiveMQ authentication on server side for REST/Ajax clients. Currently, for REST or Ajax clients to connect to a secured ActiveMQ, the client side needs to send the activemq credentials over HTTP.It should be possible to use a different authentication mechanism for the HTTP connection, and let the servlet use a system account to connect to ActiveMQ.Additionally, it would be great to allow ${} placeholders in the servlet context parameters, which could be resolved by system properties.
AMQP: Update JMS Outbound transformer to add the mapping compliant destination type markers The values used in the message annotation to indicate destination type are changed in the JMS over AMQP mapping and should now be applied in the outbound JMS transformer.  For now we can continue to apply the legacy annotations for the legacy QPid JMS client but should remove those in the future.   Sub-task AMQP: Update JMS Outbound transformer to add the mapping compliant destination type markers The values used in the message annotation to indicate destination type are changed in the JMS over AMQP mapping and should now be applied in the outbound JMS transformer.  For now we can continue to apply the legacy annotations for the legacy QPid JMS client but should remove those in the future.  
negative TotalMessageCount in JMX Broker MBean Starting a broker with a few messages on a queue and consuming these messages will cause the TotalMessageCount property on the Broker MBean go to a negative value. That value should never go negative. Improvement negative TotalMessageCount in JMX Broker MBean Starting a broker with a few messages on a queue and consuming these messages will cause the TotalMessageCount property on the Broker MBean go to a negative value. That value should never go negative.
Allow advisory messages to traverse a broker network Currently the filter applied to forwarding consumers is very restrictive. It will suppress all advisory messages. But only two types of advisory are used by the network bridge.Allowing the propagation of selective advisories, like new connection advisories is handy for monitoring at the application level. Improvement Allow advisory messages to traverse a broker network Currently the filter applied to forwarding consumers is very restrictive. It will suppress all advisory messages. But only two types of advisory are used by the network bridge.Allowing the propagation of selective advisories, like new connection advisories is handy for monitoring at the application level.
AMQP: Add support for delete of temporary destinations Right now a Temporary Destination created from a client is only deleted when its parent connection closes.  In order to support client delete of a temporary destination we need to handle close of the link that created that destination and remove it. Sub-task AMQP: Add support for delete of temporary destinations Right now a Temporary Destination created from a client is only deleted when its parent connection closes.  In order to support client delete of a temporary destination we need to handle close of the link that created that destination and remove it.
Provide a way to disable durable subscriptions from configuration. When virtual topics are used exclusively - so each topic sub gets a shared or individual queue - it would be nice to be able to enforce that regular durable subs are not allowed and reject them with an exception.A broker boolean attribute: rejectDurableConsumers would be perfect. New Feature Provide a way to disable durable subscriptions from configuration. When virtual topics are used exclusively - so each topic sub gets a shared or individual queue - it would be nice to be able to enforce that regular durable subs are not allowed and reject them with an exception.A broker boolean attribute: rejectDurableConsumers would be perfect.
Unit tests cleanup Many of the existing test cases take longer than they should and fail randomly due to BrokerService instances that are still running that have open JMX connectors etc.  We should take some time to review tests and clean up were we can.  Removing hard sleeps and using admin view to check statistics can dramatically shorten a test.  Also the tests can be configured not to create JMX connectors as none of them need that in the normal case.  Other tasks that could be done is to update tests to JUnit 4 based tests with test timeouts to help prevent CI hangs.   Improvement Unit tests cleanup Many of the existing test cases take longer than they should and fail randomly due to BrokerService instances that are still running that have open JMX connectors etc.  We should take some time to review tests and clean up were we can.  Removing hard sleeps and using admin view to check statistics can dramatically shorten a test.  Also the tests can be configured not to create JMX connectors as none of them need that in the normal case.  Other tasks that could be done is to update tests to JUnit 4 based tests with test timeouts to help prevent CI hangs.  
AMQP: Remove direct usage of types from the legacy QPid JMS 1.0 client from the tests The current AMQP tests use the QPid 1.0 JMS client and reference types from that library directly instead of using the JMS equivalents.  I light of the upcoming JMS 1.0 client that implements the JMS Mapping for AMQP we should remove these and make it easier to swap in the newer client once it is released, or test both using the same tests.   Sub-task AMQP: Remove direct usage of types from the legacy QPid JMS 1.0 client from the tests The current AMQP tests use the QPid 1.0 JMS client and reference types from that library directly instead of using the JMS equivalents.  I light of the upcoming JMS 1.0 client that implements the JMS Mapping for AMQP we should remove these and make it easier to swap in the newer client once it is released, or test both using the same tests.  
Declare variables as ConcurrentMap not ConcurrentHashMap to avoid issues after compiling on Java 8 Java 8 redefines the method keySet() on ConcurrentHashMap to return a narrower type than the standard return type given in the Map interface. If the code is compiled in Java 8 and then run under Java 7 this causes linking issues.  We don't depend on the narrowed type and can live with the standard for our usages of ConcurrentHashMapWe can avoid these issues by always declaring the instances of ConcurrentHashMap where we need method like putIfAbsent in terms of the interface ConcurrentMap Improvement Declare variables as ConcurrentMap not ConcurrentHashMap to avoid issues after compiling on Java 8 Java 8 redefines the method keySet() on ConcurrentHashMap to return a narrower type than the standard return type given in the Map interface. If the code is compiled in Java 8 and then run under Java 7 this causes linking issues.  We don't depend on the narrowed type and can live with the standard for our usages of ConcurrentHashMapWe can avoid these issues by always declaring the instances of ConcurrentHashMap where we need method like putIfAbsent in terms of the interface ConcurrentMap
AMQP: Connection open pump to socket twice in the error case when only once is needed. Should rearrange the code to do only one pump of the proton state once it's checked the full state of the connection response.  Sub-task AMQP: Connection open pump to socket twice in the error case when only once is needed. Should rearrange the code to do only one pump of the proton state once it's checked the full state of the connection response. 
Support message expiration in DLQ Currently messages in the DLQ don't expire. With the option to expire, only timely messages remain and can be processes.Immediate expiry today can be achieved on a per destination basis with the discarding strategy of with the discarding dlq broker plugin.Using message expiry is a little more intuitive and more useful, because stale messages will be auto removed from the ops radar and timely messages can be dealt with as appropriate. Improvement Support message expiration in DLQ Currently messages in the DLQ don't expire. With the option to expire, only timely messages remain and can be processes.Immediate expiry today can be achieved on a per destination basis with the discarding strategy of with the discarding dlq broker plugin.Using message expiry is a little more intuitive and more useful, because stale messages will be auto removed from the ops radar and timely messages can be dealt with as appropriate.
MQTT Tests can be sped up with some minimal changes With just a small bit of effort the MQTT tests can be sped up using more sensible wait times and stateful checks for expected outcomes.   Improvement MQTT Tests can be sped up with some minimal changes With just a small bit of effort the MQTT tests can be sped up using more sensible wait times and stateful checks for expected outcomes.  
AMQP: Update the AMQP 1.0 JMS client with newest QPid-JMS Release Upon its release we should update the QPid AMQP 1.0 JMS library to use the new QPid JMS 0.2.0 client that has recently been released.  This client is the future JMS client from the QPid project that implements the AMQP JMS spec to date and will continue to follow developments in tat specification.   Sub-task AMQP: Update the AMQP 1.0 JMS client with newest QPid-JMS Release Upon its release we should update the QPid AMQP 1.0 JMS library to use the new QPid JMS 0.2.0 client that has recently been released.  This client is the future JMS client from the QPid project that implements the AMQP JMS spec to date and will continue to follow developments in tat specification.  
AMQP: Clean up unit test start / stop code, excessive thread creation casuse slowdowns. The current test support code uses single thread executor instances as defense against hangs on test tearDown but is a bit aggressive with it as it also does Broker start / stop in an executor and never reuses them so every test run requires three to four threads just to start stop.  There's also some thread interruption bits that blindly interrupt anything that is not the main thread which could lead to problems with the JUnit threads etc.  We can reduce this all down to one executor to close the legacy QPid JMS client connection which does sometimes hang on close. This shaves off anywhere from 1.5 to 2 minutes from the test run.  Sub-task AMQP: Clean up unit test start / stop code, excessive thread creation casuse slowdowns. The current test support code uses single thread executor instances as defense against hangs on test tearDown but is a bit aggressive with it as it also does Broker start / stop in an executor and never reuses them so every test run requires three to four threads just to start stop.  There's also some thread interruption bits that blindly interrupt anything that is not the main thread which could lead to problems with the JUnit threads etc.  We can reduce this all down to one executor to close the legacy QPid JMS client connection which does sometimes hang on close. This shaves off anywhere from 1.5 to 2 minutes from the test run. 
Consider preallocation of journal files in batch increments Right now (as of ActiveMQ 5.12 release) we preallocate journal files, but the only scope is for entire journal file. The [potential] issue with that is if user configures large journal file sizes, we can end up stalling writes during log rotation because of the allocation process. There are two ways to do the allocation, configurable to do it in userspace, or defer to kernel space, but nevertheless it would be good to avoid this issue altogether by preallocating in small batch sizes regardless of the journal max file size. Improvement Consider preallocation of journal files in batch increments Right now (as of ActiveMQ 5.12 release) we preallocate journal files, but the only scope is for entire journal file. The [potential] issue with that is if user configures large journal file sizes, we can end up stalling writes during log rotation because of the allocation process. There are two ways to do the allocation, configurable to do it in userspace, or defer to kernel space, but nevertheless it would be good to avoid this issue altogether by preallocating in small batch sizes regardless of the journal max file size.
AMQP: Create simple Proton based client to be used in testing The current AMQP implementation depends on testing using the prototype JMS 1.0 client from QPid.  This present a few difficulties when it comes to unit testing the AMQP bits in that it is quite difficult to test the AMQP functionality directly as the client attempts to map AMQP to JMS but that is not the only valid type of client interaction nor is it compliant with the developing JMS mapping work.We should develop a thin proton wrapper client that lets us more easily recreate various client interactions with the broker and test compliance with AMQP 1.0 expected behaviors.   Sub-task AMQP: Create simple Proton based client to be used in testing The current AMQP implementation depends on testing using the prototype JMS 1.0 client from QPid.  This present a few difficulties when it comes to unit testing the AMQP bits in that it is quite difficult to test the AMQP functionality directly as the client attempts to map AMQP to JMS but that is not the only valid type of client interaction nor is it compliant with the developing JMS mapping work.We should develop a thin proton wrapper client that lets us more easily recreate various client interactions with the broker and test compliance with AMQP 1.0 expected behaviors.  
AMQP: clean up durable subscription unsubscribe handling to simplify and resolve issues The current unsubscribe processing is identifying a 'null-source lookup' attach as indication to immediately unsubscribe a durable subscription, performing some trickery with capabilities. There are actually many other reasons reasons a client might do this type of attach, which might lead to prematurely ending a subscription. Fully closing (rather than a non-closing detach) the durable subscription link is to be the signal ending the subscription. The related code should be fixed to resolve this, and generally cleaned up to simplify things. Sub-task AMQP: clean up durable subscription unsubscribe handling to simplify and resolve issues The current unsubscribe processing is identifying a 'null-source lookup' attach as indication to immediately unsubscribe a durable subscription, performing some trickery with capabilities. There are actually many other reasons reasons a client might do this type of attach, which might lead to prematurely ending a subscription. Fully closing (rather than a non-closing detach) the durable subscription link is to be the signal ending the subscription. The related code should be fixed to resolve this, and generally cleaned up to simplify things.
Remove the deprecated JMS streams code The JMS Streams code has been deprecated now for over a year since AMQ-4839.  We should now be able to remove this code for the 5.12 release.   Task Remove the deprecated JMS streams code The JMS Streams code has been deprecated now for over a year since AMQ-4839.  We should now be able to remove this code for the 5.12 release.  
Cleanup tests in activemq-jms-pool to fix some failures seen in Jenkins There have been a couple random failures from activemq-jms-pool tests due to not being able to bind to a JMX port.  None of these tests need to create an MBean server so we can disable that for all tests.  Also the tests don't have timeouts on all of them so we should add that as well. 1. Update all tests to JUnit 4 style tests with timeouts2. Disable embedded broker features not needed should speed things up. Improvement Cleanup tests in activemq-jms-pool to fix some failures seen in Jenkins There have been a couple random failures from activemq-jms-pool tests due to not being able to bind to a JMX port.  None of these tests need to create an MBean server so we can disable that for all tests.  Also the tests don't have timeouts on all of them so we should add that as well. 1. Update all tests to JUnit 4 style tests with timeouts2. Disable embedded broker features not needed should speed things up.
AMQP: Implement support for Temporary Topics The current implementation only supports Temporary Queue destinations.  Following the definition of the JMS mapping for AMQP we can add support for Temporary Topics as well.  Sub-task AMQP: Implement support for Temporary Topics The current implementation only supports Temporary Queue destinations.  Following the definition of the JMS mapping for AMQP we can add support for Temporary Topics as well. 
AMQP: Pull in Proton JMS transformer code and refactor to enable both JMS mapping and support for AMQP clients We currently use the Proton-JMS library to help with conversions of AMQP messages and creating destinations from links and message To fields.  This works to a point but becomes inflexible when trying to deal with the variety of clients that are interacting with the broker and the upcoming JMS mapping spec adds complexity to this.  We need to be more agile with fixes and support the old JMS client were possible.  Waiting on Proton releases and trying to create a one size fits all options for the mapping has proven ineffective.  We should use the current JMS transformation code as a base to better implement AMQP support in the broker and allow for faster fixes to issues as they arise.   Sub-task AMQP: Pull in Proton JMS transformer code and refactor to enable both JMS mapping and support for AMQP clients We currently use the Proton-JMS library to help with conversions of AMQP messages and creating destinations from links and message To fields.  This works to a point but becomes inflexible when trying to deal with the variety of clients that are interacting with the broker and the upcoming JMS mapping spec adds complexity to this.  We need to be more agile with fixes and support the old JMS client were possible.  Waiting on Proton releases and trying to create a one size fits all options for the mapping has proven ineffective.  We should use the current JMS transformation code as a base to better implement AMQP support in the broker and allow for faster fixes to issues as they arise.  
AMQP Implement the JMS Mapping spec as it evolves. The AMQP JMS Mapping specification is currently under development at OASIS.  This issue covers work to be done to implement the mechanics necessary on the broker side to allow for JMS clients that operate over AMQP.   Improvement AMQP Implement the JMS Mapping spec as it evolves. The AMQP JMS Mapping specification is currently under development at OASIS.  This issue covers work to be done to implement the mechanics necessary on the broker side to allow for JMS clients that operate over AMQP.  
Reduce the time to run STOMP tests where possible. Clean up in the STOMP tests, reduce use of sleep vs broker stats checking and use receipts when possible.   Improvement Reduce the time to run STOMP tests where possible. Clean up in the STOMP tests, reduce use of sleep vs broker stats checking and use receipts when possible.  
AMQP Module tests run much longer than necessary.   The tests in the AMQP module take a lot longer than is needed in many cases and can be tuned quite a bit to make running the tests less painful.  We can use broker stats more intelligently and reduce sleep times and receive times in many of the tests.   Improvement AMQP Module tests run much longer than necessary.   The tests in the AMQP module take a lot longer than is needed in many cases and can be tuned quite a bit to make running the tests less painful.  We can use broker stats more intelligently and reduce sleep times and receive times in many of the tests.  
AMQP shutdown transport if no connection attempt received after a configurable delay. When a socket level connection is made to the AMQP TransportConnector where the client end does not initiate a new AMQP connection we should shutdown the Transport after a configurable inactivity duration (default at 30sec). A similar feature was added for MQTT in AMQ-5468 Improvement AMQP shutdown transport if no connection attempt received after a configurable delay. When a socket level connection is made to the AMQP TransportConnector where the client end does not initiate a new AMQP connection we should shutdown the Transport after a configurable inactivity duration (default at 30sec). A similar feature was added for MQTT in AMQ-5468
preallocate journal files Our journals are append only, however we use the size to track journal rollover on recovery and replay. We can improve performance if we never update the size on disk and preallocate on creation.Rework journal logic to ensure size is never updated. This will allow the configuration option from https://issues.apache.org/jira/browse/AMQ-4947 to be the default. Improvement preallocate journal files Our journals are append only, however we use the size to track journal rollover on recovery and replay. We can improve performance if we never update the size on disk and preallocate on creation.Rework journal logic to ensure size is never updated. This will allow the configuration option from https://issues.apache.org/jira/browse/AMQ-4947 to be the default.
OSGi import ranges for javax.jms should include version 2.0 The import ranges for the ActiveMQ client bundles should allow, not exclude version 2.0 of the javax.jms API, since JMS 2.0 is backwards compatible with 1.1.  This allows for ActiveMQ client jars to be used in runtimes with other JMS 2.0 clients.Should be:javax.jms;version=[1.1,2] instead of:javax.jms;version=[1.1,2) Improvement OSGi import ranges for javax.jms should include version 2.0 The import ranges for the ActiveMQ client bundles should allow, not exclude version 2.0 of the javax.jms API, since JMS 2.0 is backwards compatible with 1.1.  This allows for ActiveMQ client jars to be used in runtimes with other JMS 2.0 clients.Should be:javax.jms;version=[1.1,2] instead of:javax.jms;version=[1.1,2)
Configurable messages size for Stomp producer It is coming from http://activemq.2283324.n4.nabble.com/ActiveMQ-Stomp-producer-messages-size-limitation-td4689847.html#a4691046 Stomp APIs does not allow to send message more than 100MB. These parameters are in (http://activemq.apache.org/maven/apidocs/src-html/org/apache/activemq/transport/stomp/StompWireFormat.html)  'MAX_DATA_LENGTH' and 'MAX_COMMAND_LENGTH' .There might be cases when producer want to send more than 100MB of message. It's better to have such parameters configurable.?Thanks,Anuj Improvement Configurable messages size for Stomp producer It is coming from http://activemq.2283324.n4.nabble.com/ActiveMQ-Stomp-producer-messages-size-limitation-td4689847.html#a4691046 Stomp APIs does not allow to send message more than 100MB. These parameters are in (http://activemq.apache.org/maven/apidocs/src-html/org/apache/activemq/transport/stomp/StompWireFormat.html)  'MAX_DATA_LENGTH' and 'MAX_COMMAND_LENGTH' .There might be cases when producer want to send more than 100MB of message. It's better to have such parameters configurable.?Thanks,Anuj
AMQP: use type descriptors to inspect AMQP filter types rather than the map keys The broker is currently looking up AMQP filters using named keys of the filter map. This approach is fragile, since difference clients may name equivalent filters differently, as the names are not intended to identify particular types. The broker should instead inspect the descriptor values when trying to identify the particular type of a filter. Sub-task AMQP: use type descriptors to inspect AMQP filter types rather than the map keys The broker is currently looking up AMQP filters using named keys of the filter map. This approach is fragile, since difference clients may name equivalent filters differently, as the names are not intended to identify particular types. The broker should instead inspect the descriptor values when trying to identify the particular type of a filter.
introduce a smoke-test profile that is enabled by default and during release:prepare Users should be able to do $>mvn install on trunk or the source distribution and get a validated (smoke-tested) distribution in < 10  minutes.The smoke-test profile should be enabled for release:prepareAt the moment, more than 3k tests are run, they are not reliable enough and the build is gone for a number of hours. This gives a bad first impression.Or course we should continue to improve the test suite but this has a totally different focus.The smoke-test profile takes a smart cross section of tests in each module that validate core functionality. It will be an interesting challenge to get the selection right; balancing typical use cases with coverage with speed etc.The tests should be: * representative of the module functionality * clean - no hard-coded ports and use only space on target * fast * reliable * can be run in parallel (maybe if it allows more tests to be run in the same time frame)  Improvement introduce a smoke-test profile that is enabled by default and during release:prepare Users should be able to do $>mvn install on trunk or the source distribution and get a validated (smoke-tested) distribution in < 10  minutes.The smoke-test profile should be enabled for release:prepareAt the moment, more than 3k tests are run, they are not reliable enough and the build is gone for a number of hours. This gives a bad first impression.Or course we should continue to improve the test suite but this has a totally different focus.The smoke-test profile takes a smart cross section of tests in each module that validate core functionality. It will be an interesting challenge to get the selection right; balancing typical use cases with coverage with speed etc.The tests should be: * representative of the module functionality * clean - no hard-coded ports and use only space on target * fast * reliable * can be run in parallel (maybe if it allows more tests to be run in the same time frame) 
Support preemptive redelivery flag for non persistent messages policy entry setPersistJMSRedelivered gives us a hardened redelivery flag even in the event of a restarthttps://issues.apache.org/jira/browse/AMQ-5068This feature is conditional on the message being persistent but it need not be. For non persistent messages, we can eagerly set the redelivered flag. This is useful for connection drops and or stomp clients that close their socket. If they ever care about their delivery flag. Improvement Support preemptive redelivery flag for non persistent messages policy entry setPersistJMSRedelivered gives us a hardened redelivery flag even in the event of a restarthttps://issues.apache.org/jira/browse/AMQ-5068This feature is conditional on the message being persistent but it need not be. For non persistent messages, we can eagerly set the redelivered flag. This is useful for connection drops and or stomp clients that close their socket. If they ever care about their delivery flag.
upgrade to karaf 2.4.1 matching camel, seems more stable in the itests w.r.t to hangs at shutdown. Improvement upgrade to karaf 2.4.1 matching camel, seems more stable in the itests w.r.t to hangs at shutdown.
Average message size attribute on statistics plugin should not have decimals Related to AMQ-5521The avg message size should be long based, as also presented in JMX Improvement Average message size attribute on statistics plugin should not have decimals Related to AMQ-5521The avg message size should be long based, as also presented in JMX
Average message size attribute on destination mbean should not have decimals Under AMQ-4831, we introduced rounding on the AverageMessageSize attribute of BrokerViewMBean, but the same change wasn't applied to the same attribute on DestinationViewMBean so the two are inconsistent.  If the change was worth making, it should be made both places. Improvement Average message size attribute on destination mbean should not have decimals Under AMQ-4831, we introduced rounding on the AverageMessageSize attribute of BrokerViewMBean, but the same change wasn't applied to the same attribute on DestinationViewMBean so the two are inconsistent.  If the change was worth making, it should be made both places.
upgrade to jetty 8 In prep for release I found:{code}[WARNING] The POM for org.eclipse.jetty:maven-jetty-plugin:jar:7.6.9.v20130131 is missing, no dependency information available{code}7.x is out of maintenance and obviously missing some bits from maven central.Going to pull in servlet 3 and jetty 8. Improvement upgrade to jetty 8 In prep for release I found:{code}[WARNING] The POM for org.eclipse.jetty:maven-jetty-plugin:jar:7.6.9.v20130131 is missing, no dependency information available{code}7.x is out of maintenance and obviously missing some bits from maven central.Going to pull in servlet 3 and jetty 8.
ActiveMQSslConnectionFactory should support different keystore and key passwords The current ActiveMQSslConnectionFactory allows us to specify- trustStore- trustStorePassword- keyStore- keyStorePasswordIn case the passphrase of the key contained in the keystore is different from the password of the keystore itself this connection factory cannot be used.What we're missing is the following field- keyStoreKeyPasswordIn the ActiveMQ configuration such a setup is supported (via the [SpringSslContext|https://svn.apache.org/repos/asf/activemq/trunk/activemq-spring/src/main/java/org/apache/activemq/spring/SpringSslContext.java]), however for accessing ActiveMQ via the ActiveMQSslConnectionFactory it is not.Adding a keyStoreKeyPassword field and changing the createKeyManager slightly would fix this:{noformat}    protected KeyManager[] createKeyManager() throws Exception {    	        KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());        KeyStore ks = KeyStore.getInstance("jks");        KeyManager[] keystoreManagers = null;        if (keyStore != null) {            byte[] sslCert = loadClientCredential(keyStore);            if (sslCert != null && sslCert.length > 0) {                ByteArrayInputStream bin = new ByteArrayInputStream(sslCert);                ks.load(bin, keyStorePassword.toCharArray());                kmf.init(ks, keyStoreKeyPassword !=null ? keyStoreKeyPassword.toCharArray() : keyStorePassword.toCharArray());                keystoreManagers = kmf.getKeyManagers();            }        }        return keystoreManagers;    }{noformat} Improvement ActiveMQSslConnectionFactory should support different keystore and key passwords The current ActiveMQSslConnectionFactory allows us to specify- trustStore- trustStorePassword- keyStore- keyStorePasswordIn case the passphrase of the key contained in the keystore is different from the password of the keystore itself this connection factory cannot be used.What we're missing is the following field- keyStoreKeyPasswordIn the ActiveMQ configuration such a setup is supported (via the [SpringSslContext|https://svn.apache.org/repos/asf/activemq/trunk/activemq-spring/src/main/java/org/apache/activemq/spring/SpringSslContext.java]), however for accessing ActiveMQ via the ActiveMQSslConnectionFactory it is not.Adding a keyStoreKeyPassword field and changing the createKeyManager slightly would fix this:{noformat}    protected KeyManager[] createKeyManager() throws Exception {    	        KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());        KeyStore ks = KeyStore.getInstance("jks");        KeyManager[] keystoreManagers = null;        if (keyStore != null) {            byte[] sslCert = loadClientCredential(keyStore);            if (sslCert != null && sslCert.length > 0) {                ByteArrayInputStream bin = new ByteArrayInputStream(sslCert);                ks.load(bin, keyStorePassword.toCharArray());                kmf.init(ks, keyStoreKeyPassword !=null ? keyStoreKeyPassword.toCharArray() : keyStorePassword.toCharArray());                keystoreManagers = kmf.getKeyManagers();            }        }        return keystoreManagers;    }{noformat}
Trace logs in MQTT Protocol Converter Adding trace messages in MQTT Protocol Converter for easier debugging. Improvement Trace logs in MQTT Protocol Converter Adding trace messages in MQTT Protocol Converter for easier debugging.
Provider fine-grained control for SelectorManager's threadpool In SelectorManager::createDefaultExecutor, the ThreadPoolExecutor is created with Integer.MAX_VALUE as MaximumPoolSize. This does not work well under our scenarios. It would be nicer to give fine-control over CorePoolSize, MaximumPoolSize  as what's been done for KeepAlive Time Improvement Provider fine-grained control for SelectorManager's threadpool In SelectorManager::createDefaultExecutor, the ThreadPoolExecutor is created with Integer.MAX_VALUE as MaximumPoolSize. This does not work well under our scenarios. It would be nicer to give fine-control over CorePoolSize, MaximumPoolSize  as what's been done for KeepAlive Time
bin/activemq batch script error message prints wrong variable bin/activemq line 334:{code}  echo "       (JAVA_HOME='$JAVAHOME', JAVACMD='$JAVACMD')"{code}This line prints the value of the variable {$JAVAHOME} instead of {$JAVA_HOME}.I think the line should be changed to{code}  echo "       (JAVA_HOME='$JAVA_HOME', JAVACMD='$JAVACMD')"{code} Improvement bin/activemq batch script error message prints wrong variable bin/activemq line 334:{code}  echo "       (JAVA_HOME='$JAVAHOME', JAVACMD='$JAVACMD')"{code}This line prints the value of the variable {$JAVAHOME} instead of {$JAVA_HOME}.I think the line should be changed to{code}  echo "       (JAVA_HOME='$JAVA_HOME', JAVACMD='$JAVACMD')"{code}
AMQP - delayed authentication from SASL connect leads to race on client end. We currently delay checking the credentials provided during the SASL negotiation and also checking if anonymous client connects are legal until after opening the proton connection and then we send an error condition indicating the failure and close the connection.  This can lead to a race on the client end where it looks for a breif moment in time that the connection succeeded.  During that time the client might attempt some further action and then fail in an odd way as the connection is closed under it.  We should look into authenticating immediately and failing the SASL handshake if not authorized.  We should also consider whether we want to support raw connections with a SASL handshake as well since without at least a SASL ANONYMOUS handshake we can get back into this issue unless we just forcibly close the socket on a client if we don't support anonymous connections. Sub-task AMQP - delayed authentication from SASL connect leads to race on client end. We currently delay checking the credentials provided during the SASL negotiation and also checking if anonymous client connects are legal until after opening the proton connection and then we send an error condition indicating the failure and close the connection.  This can lead to a race on the client end where it looks for a breif moment in time that the connection succeeded.  During that time the client might attempt some further action and then fail in an odd way as the connection is closed under it.  We should look into authenticating immediately and failing the SASL handshake if not authorized.  We should also consider whether we want to support raw connections with a SASL handshake as well since without at least a SASL ANONYMOUS handshake we can get back into this issue unless we just forcibly close the socket on a client if we don't support anonymous connections.
Performance Test does not cater for temporary destinations The current version of the performance test tool does not support the use of temporary queues or topics. Consumers should be able to create one on demand, while producers should be able to send messages to a named temp destination. Improvement Performance Test does not cater for temporary destinations The current version of the performance test tool does not support the use of temporary queues or topics. Consumers should be able to create one on demand, while producers should be able to send messages to a named temp destination.
Support of jms.consumerExpiryCheck=false to avoid JMS Consumers ignoring some messages in case of out-of-synch clocks AS IS JMS Consumer re-checks messages served to him by the Broker and ignores those appearing to be expired according to the clock on the Consumer machine.TO BE Allow Consumer to optionally disable this check and blindly trust the expirations check already performed at the Broker, according to the broker clock.RATIONALESWhen the Consumer clock is running ahead of the Broker clock by more that the time-to-live of a message, the message would be wrongly ignored by the consumer.Please notice that in a situation like this BrokerTimestampingPlugin is of no help, because it only influence the way messages sent by the Producer are handled at the broker, while the issue reported here is Consumer related.ADDITIONAL RATIONALES1\ STOMP API does NOT perform expiration re-checks at the consumer (and this is good)2\ .NET API already has an option as the one request here and for similar reasonsBACKGROUND DISCUSSIONHere are some snippets of the background discussion on users@activemq.apache.org list, where this request has been informally seconded by Gary Tully & Timothy Bish (thanks guys!):http://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCANGe49ckNM-6_DiYWVTtDfgAZPJ2SJoMc3ypJF7K+VsBdoUaVA@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCAFitrpQJK4MukyJ5yxPb=ygQnC0yV5jetgWiesxu8BqagfSScQ@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCANGe49c0Xok732URRZKAx0__H9XVqjys_7auE5ysH0um+4fZkQ@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCAH+vQmPtedrsomaZsvzU+eksLEm6GGeO5p=QzYPtqwB7ArvgAw@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3C5447BF6F.1090103@gmail.com%3E Improvement Support of jms.consumerExpiryCheck=false to avoid JMS Consumers ignoring some messages in case of out-of-synch clocks AS IS JMS Consumer re-checks messages served to him by the Broker and ignores those appearing to be expired according to the clock on the Consumer machine.TO BE Allow Consumer to optionally disable this check and blindly trust the expirations check already performed at the Broker, according to the broker clock.RATIONALESWhen the Consumer clock is running ahead of the Broker clock by more that the time-to-live of a message, the message would be wrongly ignored by the consumer.Please notice that in a situation like this BrokerTimestampingPlugin is of no help, because it only influence the way messages sent by the Producer are handled at the broker, while the issue reported here is Consumer related.ADDITIONAL RATIONALES1\ STOMP API does NOT perform expiration re-checks at the consumer (and this is good)2\ .NET API already has an option as the one request here and for similar reasonsBACKGROUND DISCUSSIONHere are some snippets of the background discussion on users@activemq.apache.org list, where this request has been informally seconded by Gary Tully & Timothy Bish (thanks guys!):http://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCANGe49ckNM-6_DiYWVTtDfgAZPJ2SJoMc3ypJF7K+VsBdoUaVA@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCAFitrpQJK4MukyJ5yxPb=ygQnC0yV5jetgWiesxu8BqagfSScQ@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCANGe49c0Xok732URRZKAx0__H9XVqjys_7auE5ysH0um+4fZkQ@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3CCAH+vQmPtedrsomaZsvzU+eksLEm6GGeO5p=QzYPtqwB7ArvgAw@mail.gmail.com%3Ehttp://mail-archives.apache.org/mod_mbox/activemq-users/201410.mbox/%3C5447BF6F.1090103@gmail.com%3E
enable support for using byte values in destination type annotations Add support for using the improved destination type annotation value support introduced in PROTON-711.This will be disabled by default for compatibility, being enabled either by the client indicating a connection property value for 'x-opt-jms-mapping-version', or the wireFormat config being used to turn it on for all connections. Improvement enable support for using byte values in destination type annotations Add support for using the improved destination type annotation value support introduced in PROTON-711.This will be disabled by default for compatibility, being enabled either by the client indicating a connection property value for 'x-opt-jms-mapping-version', or the wireFormat config being used to turn it on for all connections.
Update disk based limits periodically At the moment, we set store and temp limits at broker startup based on the configuration and available space. It's possible that other artefacts such as logs can reduce available disk space so that our limits does not have effect. It'd be good to periodically check for the usable space left and adjust limits accordingly. New Feature Update disk based limits periodically At the moment, we set store and temp limits at broker startup based on the configuration and available space. It's possible that other artefacts such as logs can reduce available disk space so that our limits does not have effect. It'd be good to periodically check for the usable space left and adjust limits accordingly.
Support for anonymous style producers in AMQP Current AMQP implementation doesn't provide a way to implement efficient anonymous producers.  The QPid JMS client for instance opens a new producer instance for each destination that the anonymous producer sends to.  To more efficiently handle anonymous producers we should provide a configurable node name (default as $relay) which would use the to feild of the incoming message to decide where the message is to be sent.   New Feature Support for anonymous style producers in AMQP Current AMQP implementation doesn't provide a way to implement efficient anonymous producers.  The QPid JMS client for instance opens a new producer instance for each destination that the anonymous producer sends to.  To more efficiently handle anonymous producers we should provide a configurable node name (default as $relay) which would use the to feild of the incoming message to decide where the message is to be sent.  
Upgrade to Camel 2.14 Camel 2.14 is released. Task Upgrade to Camel 2.14 Camel 2.14 is released.
Update Proton to version 0.8 As part of the next release we will want to move up to Proton v0.8 which will incorporate several fixes as well as improvements in the codec speed and memory usage.  For now we should move onto the published 1.0-SNAPSHOT builds and start testing to find any issues that fall out from the upgrade.  Dejan has already created a patch that deals with the API changes. Improvement Update Proton to version 0.8 As part of the next release we will want to move up to Proton v0.8 which will incorporate several fixes as well as improvements in the codec speed and memory usage.  For now we should move onto the published 1.0-SNAPSHOT builds and start testing to find any issues that fall out from the upgrade.  Dejan has already created a patch that deals with the API changes.
Validate priorityBackupURIs against list of failover URIs The following priority back up config does not work:{code} failover:(tcp://localhost:1234,tcp://primary1:1234,tcp://secondary1:1234)?                        nested.socket.tcpNoDelay=true&                        nested.soTimeout=60000&                        nested.closeAsync=false&                        randomize=false&                        priorityBackup=true&                        priorityURIs=tcp://localhost:1234{code}This is because the full connected URI is used when testing whether the  current connection is the priority connection. In the above example the nested socket params are added to the actual connected URI which then does not match tcp://localhost:1234 as setup in the priority list.This causes the transport to continually try to reconnect to the primary, even though it is already connected to it.Can it either take nested parameters into account so the above works as expected (it looks like a sensible URL to me) and/or validate that each of the priorityBackupURIs are valid URIs in the failover list Improvement Validate priorityBackupURIs against list of failover URIs The following priority back up config does not work:{code} failover:(tcp://localhost:1234,tcp://primary1:1234,tcp://secondary1:1234)?                        nested.socket.tcpNoDelay=true&                        nested.soTimeout=60000&                        nested.closeAsync=false&                        randomize=false&                        priorityBackup=true&                        priorityURIs=tcp://localhost:1234{code}This is because the full connected URI is used when testing whether the  current connection is the priority connection. In the above example the nested socket params are added to the actual connected URI which then does not match tcp://localhost:1234 as setup in the priority list.This causes the transport to continually try to reconnect to the primary, even though it is already connected to it.Can it either take nested parameters into account so the above works as expected (it looks like a sensible URL to me) and/or validate that each of the priorityBackupURIs are valid URIs in the failover list
activemq-pool - Should not log expired connection when the pool is stopped as that causes log floods during shutdown If you shutdown amq client + pool and whatnot, then you may get excessive logs about expired connections, as they are logged at INFO level.Example{code}2014-08-07 14:23:42,572 [est-broker-2#16] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:1,clientId=ID:davsclaus.air-50586-1407414219217-4:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#17) stopped.2014-08-07 14:23:42,575 [est-broker-3#32] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:1,clientId=ID:davsclaus.air-50586-1407414219217-6:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#33) stopped.2014-08-07 14:23:42,575 [test-broker-1#4] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:3,clientId=ID:davsclaus.air-50586-1407414219217-2:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#5) stopped.2014-08-07 14:23:42,576 [test-broker-1#0] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:1,clientId=ID:davsclaus.air-50586-1407414219217-2:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#1) stopped.2014-08-07 14:23:42,577 [est-broker-2#18] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:2,clientId=ID:davsclaus.air-50586-1407414219217-4:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#19) stopped.2014-08-07 14:23:42,577 [est-broker-3#38] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:4,clientId=ID:davsclaus.air-50586-1407414219217-6:4,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#39) stopped.2014-08-07 14:23:42,578 [test-broker-1#6] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:4,clientId=ID:davsclaus.air-50586-1407414219217-2:4,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#7) stopped.2014-08-07 14:23:42,578 [est-broker-3#36] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:3,clientId=ID:davsclaus.air-50586-1407414219217-6:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#37) stopped.2014-08-07 14:23:42,579 [test-broker-1#2] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:2,clientId=ID:davsclaus.air-50586-1407414219217-2:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#3) stopped.2014-08-07 14:23:42,579 [test-broker-1#8] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:5,clientId=ID:davsclaus.air-50586-1407414219217-2:5,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#9) stopped.2014-08-07 14:23:42,579 [est-broker-3#34] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:2,clientId=ID:davsclaus.air-50586-1407414219217-6:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#35) stopped.2014-08-07 14:23:42,580 [est-broker-1#10] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:6,clientId=ID:davsclaus.air-50586-1407414219217-2:6,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#11) stopped.2014-08-07 14:23:42,581 [est-broker-2#20] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:3,clientId=ID:davsclaus.air-50586-1407414219217-4:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#21) stopped.2014-08-07 14:23:42,582 [est-broker-3#40] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:5,clientId=ID:davsclaus.air-50586-1407414219217-6:5,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#41) stopped.2014-08-07 14:23:42,582 [MQ ShutdownHook] INFO  TransportConnector             - Connector vm://test-broker-1 stopped{code}We should reduce this noise and only log if we are not stopped. Improvement activemq-pool - Should not log expired connection when the pool is stopped as that causes log floods during shutdown If you shutdown amq client + pool and whatnot, then you may get excessive logs about expired connections, as they are logged at INFO level.Example{code}2014-08-07 14:23:42,572 [est-broker-2#16] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:1,clientId=ID:davsclaus.air-50586-1407414219217-4:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#17) stopped.2014-08-07 14:23:42,575 [est-broker-3#32] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:1,clientId=ID:davsclaus.air-50586-1407414219217-6:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#33) stopped.2014-08-07 14:23:42,575 [test-broker-1#4] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:3,clientId=ID:davsclaus.air-50586-1407414219217-2:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#5) stopped.2014-08-07 14:23:42,576 [test-broker-1#0] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:1,clientId=ID:davsclaus.air-50586-1407414219217-2:1,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#1) stopped.2014-08-07 14:23:42,577 [est-broker-2#18] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:2,clientId=ID:davsclaus.air-50586-1407414219217-4:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#19) stopped.2014-08-07 14:23:42,577 [est-broker-3#38] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:4,clientId=ID:davsclaus.air-50586-1407414219217-6:4,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#39) stopped.2014-08-07 14:23:42,578 [test-broker-1#6] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:4,clientId=ID:davsclaus.air-50586-1407414219217-2:4,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#7) stopped.2014-08-07 14:23:42,578 [est-broker-3#36] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:3,clientId=ID:davsclaus.air-50586-1407414219217-6:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#37) stopped.2014-08-07 14:23:42,579 [test-broker-1#2] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:2,clientId=ID:davsclaus.air-50586-1407414219217-2:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#3) stopped.2014-08-07 14:23:42,579 [test-broker-1#8] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:5,clientId=ID:davsclaus.air-50586-1407414219217-2:5,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#9) stopped.2014-08-07 14:23:42,579 [est-broker-3#34] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:2,clientId=ID:davsclaus.air-50586-1407414219217-6:2,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#35) stopped.2014-08-07 14:23:42,580 [est-broker-1#10] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-3:6,clientId=ID:davsclaus.air-50586-1407414219217-2:6,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-1#11) stopped.2014-08-07 14:23:42,581 [est-broker-2#20] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-5:3,clientId=ID:davsclaus.air-50586-1407414219217-4:3,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-2#21) stopped.2014-08-07 14:23:42,582 [est-broker-3#40] INFO  PooledConnectionFactory        - Expiring connection ActiveMQConnection {id=ID:davsclaus.air-50586-1407414219217-7:5,clientId=ID:davsclaus.air-50586-1407414219217-6:5,started=false} on IOException: org.apache.activemq.transport.TransportDisposedIOException: peer (vm://test-broker-3#41) stopped.2014-08-07 14:23:42,582 [MQ ShutdownHook] INFO  TransportConnector             - Connector vm://test-broker-1 stopped{code}We should reduce this noise and only log if we are not stopped.
MQTT NIO and NIO+SSL transports can be slow when reading in larger messages The MQTTCodec class reads in message content a byte at a time which can result in very slow reads for larger messages.   Instead the code should use a buffer and fill it with as much data as it can in one shot each time a chunk of data is received.   Improvement MQTT NIO and NIO+SSL transports can be slow when reading in larger messages The MQTTCodec class reads in message content a byte at a time which can result in very slow reads for larger messages.   Instead the code should use a buffer and fill it with as much data as it can in one shot each time a chunk of data is received.  
Destination should not have numerical suffix for single-dest perf tests The performance test module assumes that multiple destinations will be put under load, and thereby assigns a numerical suffix to the destination name specified on the command line. Thus a producer/consumer configured to topic://foo will actually send/receive from topic://foo.0. This is annoying, as when load testing a particular broker setup (such as with composite destinations), you need to be explicit about which destination is being targeted and it is not always possible to tweak the broker configuration.I propose that for single-destination tests, no numerical suffix is added. Patch incoming. Improvement Destination should not have numerical suffix for single-dest perf tests The performance test module assumes that multiple destinations will be put under load, and thereby assigns a numerical suffix to the destination name specified on the command line. Thus a producer/consumer configured to topic://foo will actually send/receive from topic://foo.0. This is annoying, as when load testing a particular broker setup (such as with composite destinations), you need to be explicit about which destination is being targeted and it is not always possible to tweak the broker configuration.I propose that for single-destination tests, no numerical suffix is added. Patch incoming.
MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case.  With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.   Improvement MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case.  With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.  
Track forwards across a network in destination statistics When using message stats for accountability in a broker mesh the forwarding of messages results in duplicate accounting because the same message is dequeued multiple times, when forwarded and when consumed.tracking forwards in the destination statistic means that local consumption can be captured with dequeueCount - forwardCount Improvement Track forwards across a network in destination statistics When using message stats for accountability in a broker mesh the forwarding of messages results in duplicate accounting because the same message is dequeued multiple times, when forwarded and when consumed.tracking forwards in the destination statistic means that local consumption can be captured with dequeueCount - forwardCount
Add an in-memory JobSchedulerStore implementation For brokers that run with persistence disabled there currently no in-memory job scheduler store so an embedded broker without persistence can't have scheduled messages or do broker led redelivery without manually configuring in the normal JobSchedulerStore impl which requires a disk based store.  We should add an in memory variant that is the default if persistence is disabled but scheduler support is enabled.  New Feature Add an in-memory JobSchedulerStore implementation For brokers that run with persistence disabled there currently no in-memory job scheduler store so an embedded broker without persistence can't have scheduled messages or do broker led redelivery without manually configuring in the normal JobSchedulerStore impl which requires a disk based store.  We should add an in memory variant that is the default if persistence is disabled but scheduler support is enabled. 
Allow optional manual transaction management on a rar managedConnection  In https://github.com/apache/activemq/commit/adb49f562725087865ded9a773ee6901423c870bthe jee semantics were tightened up to ignore args passed to create session.see: http://mail-archives.apache.org/mod_mbox/activemq-users/201407.mbox/%3CCAH%2BvQmPgMYGmWB_QX8gcFqMOcFa9mGkZEDjhEiASpbUKen3f4w%40mail.gmail.com%3EIn cases where spring is uses with the rar outside of ee, it is sensible to retain the standard jms semantics and respect that args. The app is then responsible for ensuring transaction completion as normal. Improvement Allow optional manual transaction management on a rar managedConnection  In https://github.com/apache/activemq/commit/adb49f562725087865ded9a773ee6901423c870bthe jee semantics were tightened up to ignore args passed to create session.see: http://mail-archives.apache.org/mod_mbox/activemq-users/201407.mbox/%3CCAH%2BvQmPgMYGmWB_QX8gcFqMOcFa9mGkZEDjhEiASpbUKen3f4w%40mail.gmail.com%3EIn cases where spring is uses with the rar outside of ee, it is sensible to retain the standard jms semantics and respect that args. The app is then responsible for ensuring transaction completion as normal.
Failover Transport timeout option causes connection failures in some cases where it shouldn't Originally added in AMQ-2061 the timeout option is used to cause a send of Message that's gone out async to fail if the connection doesn't come back in some amount of time.  The problem is that the option is currently applied to everything that goes through FailoverTransport oneway() and this can cause a Connection start where the broker is not up to fail regardless of the maxReconnectAttempts or startupMaxReconnectAttempts value configured for the transport. We need to refine the timeout logic to only apply to Message instances and not to other commands like ConnectionInfo so that a Connection start honors the normal failover transport reconnect configuration logic.  Improvement Failover Transport timeout option causes connection failures in some cases where it shouldn't Originally added in AMQ-2061 the timeout option is used to cause a send of Message that's gone out async to fail if the connection doesn't come back in some amount of time.  The problem is that the option is currently applied to everything that goes through FailoverTransport oneway() and this can cause a Connection start where the broker is not up to fail regardless of the maxReconnectAttempts or startupMaxReconnectAttempts value configured for the transport. We need to refine the timeout logic to only apply to Message instances and not to other commands like ConnectionInfo so that a Connection start honors the normal failover transport reconnect configuration logic. 
Queue; be able to pause/resume dispatch of message to all consumers It would be good to be able to pause/resume the dispatch of messages from a queue to the queues consumers.When the queue is "paused":-  NO messages sent to the associate consumers-  messages still to be enqueued on the queue-  ability to be able to browse the queue-  all the JMX counters for the queue to be available and correct. Improvement Queue; be able to pause/resume dispatch of message to all consumers It would be good to be able to pause/resume the dispatch of messages from a queue to the queues consumers.When the queue is "paused":-  NO messages sent to the associate consumers-  messages still to be enqueued on the queue-  ability to be able to browse the queue-  all the JMX counters for the queue to be available and correct.
Security errors for sync commands are only logged at debug levels For any operation that sends a sync commands (response required) there is no warning logs indicating a security exception was triggered.  In the async case we log in detail.  We should add a log at warn level for security errors sent back as responses.   Improvement Security errors for sync commands are only logged at debug levels For any operation that sends a sync commands (response required) there is no warning logs indicating a security exception was triggered.  In the async case we log in detail.  We should add a log at warn level for security errors sent back as responses.  
Allow for changing logger levels via JMX Create a new MBean that is loaded if the Broker is running with Log4J as which would allow for changing the level of loggers via JMX to enable debug without needing access to the log4j.properties. New Feature Allow for changing logger levels via JMX Create a new MBean that is loaded if the Broker is running with Log4J as which would allow for changing the level of loggers via JMX to enable debug without needing access to the log4j.properties.
Add a statistic for showing how many messages every scheduler job triggered Is there anyway we can see how many messages are scheduled for re-delivery per queue? At the moment the best we can do is get the number of scheduled jobs via JMX, which is not queue-specific. Is it possible to programmatically query the KahaDB to inspect the scheduled redelivery jobs? Improvement Add a statistic for showing how many messages every scheduler job triggered Is there anyway we can see how many messages are scheduled for re-delivery per queue? At the moment the best we can do is get the number of scheduled jobs via JMX, which is not queue-specific. Is it possible to programmatically query the KahaDB to inspect the scheduled redelivery jobs?
Switch to using Proton's Event logic for detecting AMQP state changes We currently use a polling model to detect state changes in the proton engine when new data arrives.  The recent update to Proton v0.7.0 allows us to switch to the new Event Collector model and remove the polling code.  This change results in lower overhead when processing incoming AMQP frames and increases performance of the AMQP transport layer.   Improvement Switch to using Proton's Event logic for detecting AMQP state changes We currently use a polling model to detect state changes in the proton engine when new data arrives.  The recent update to Proton v0.7.0 allows us to switch to the new Event Collector model and remove the polling code.  This change results in lower overhead when processing incoming AMQP frames and increases performance of the AMQP transport layer.  
ActiveMQ Web Console URL log message hard coded to localhost The following message is logged even if the Jetty Server Host is set to use a specific IP:{code}INFO | ActiveMQ WebConsole available at http://localhost:8161/{code}Looking at org/apache/activemq/web/WebConsoleStarter.java, the URL is hard coded to localhost:{code}// for embedded console log what port it uses        if ("embedded".equals(webconsoleType)) {            // show the url for the web consoles / main page so people can spot it            String port = System.getProperty("jetty.port");            if (port != null) {                LOG.info("ActiveMQ WebConsole available at http://localhost:{}/", port);            }        }{code}The log message doesn't account for users setting the Jetty host property. Improvement ActiveMQ Web Console URL log message hard coded to localhost The following message is logged even if the Jetty Server Host is set to use a specific IP:{code}INFO | ActiveMQ WebConsole available at http://localhost:8161/{code}Looking at org/apache/activemq/web/WebConsoleStarter.java, the URL is hard coded to localhost:{code}// for embedded console log what port it uses        if ("embedded".equals(webconsoleType)) {            // show the url for the web consoles / main page so people can spot it            String port = System.getProperty("jetty.port");            if (port != null) {                LOG.info("ActiveMQ WebConsole available at http://localhost:{}/", port);            }        }{code}The log message doesn't account for users setting the Jetty host property.
exclude bouncycastle dependency from unit tests run The ApacheDS dependency used in the AMQ unit tests pulls in an old bouncycastle lib that creates errors in JDK 1.7 releases.  Exclude this jar from the tests.   Improvement exclude bouncycastle dependency from unit tests run The ApacheDS dependency used in the AMQ unit tests pulls in an old bouncycastle lib that creates errors in JDK 1.7 releases.  Exclude this jar from the tests.  
Enable durable topic subscriptions using individual ack mode. Updates to KahaDB for durable subscription tracking now makes it possible to use individual ack mode with durable subs.  We can now re-enable this feature which was disabled in AMQ-3486 Improvement Enable durable topic subscriptions using individual ack mode. Updates to KahaDB for durable subscription tracking now makes it possible to use individual ack mode with durable subs.  We can now re-enable this feature which was disabled in AMQ-3486
AbortSlow*ConsumerStrategy with abortConnection=false; ensure consumers are always removed from the broker  If a consumer is slow and a candidate to abort, the broker sends a control command to the consumer. However there is no guarantee that the consumer will respond to the command and close. As a result the consumer can remain longer than necessary and possibly indefinitely.,The broker needs to follow up with a local removeInfo  that will force the removal from the broker so that inflight messages can get eagerly redispatched.The client can eventually shut down the client view of the consumer but the broker will be able to continue regardless. Improvement AbortSlow*ConsumerStrategy with abortConnection=false; ensure consumers are always removed from the broker  If a consumer is slow and a candidate to abort, the broker sends a control command to the consumer. However there is no guarantee that the consumer will respond to the command and close. As a result the consumer can remain longer than necessary and possibly indefinitely.,The broker needs to follow up with a local removeInfo  that will force the removal from the broker so that inflight messages can get eagerly redispatched.The client can eventually shut down the client view of the consumer but the broker will be able to continue regardless.
Upgrade to jolokia 1.2 Jolokia 1.2.0 has been released. Improvement Upgrade to jolokia 1.2 Jolokia 1.2.0 has been released.
Improve performance of composite topic fanout and persistent asyncSend We have publishers publishing to a topic which has 5 topic -> queue routings, and gets a max message rate attainable of ~833 messages/sec, with each message around 5k in size.To test this i set up a JMS config with topic queues:TopicTopicRouted.1...TopicRouted.11Each topic has an increasing number of routings to queues, and a client is set up to subscribe to all the queues.Rough message rates:routings messages/sec0 25001 14282 20003 14284 11115 833This occurs whether the broker config has producerFlowControl="false" set to true or false , and KahaDB disk synching is turned off. We also tried experimenting with concurrentStoreAndDispatch, but that didn't seem to help. LevelDB didn't give any notable performance improvement either.We also have asyncSend enabled on the producer, and have a requirement to use persistent messages. We have also experimented with sending messages in a transaction, but that hasn't really helped.It seems like producer throughput rate across all queue destinations, all connections and all publisher machines is limited by something on the broker, through a mechanism which is not producer flow control. I think the prime suspect is still contention on the index.We did some test with Yourkit profiler.Profiler was attached to broker at startup, allowed to run and then a topic publisher was started, routing to 5 queues. Profiler statistics were reset, the publisher allowed to run for 60 seconds, and then profiling snapshot was taken. During that time, ~9600 messages were logged as being sent for a rate of ~160/sec.This ties in roughly with the invocation counts recorded in the snapshot (i think) - ~43k calls. From what i can work out, in the snapshot (filtering everything but org.apache.activemq.store.kahadb), For the 60 second sample period, 24.8 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.removeAsyncMessage(ConnectionContext, MessageAck).18.3 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.asyncAddQueueMessage(ConnectionContext, Message, boolean).From these, a further large portion of the time is spent inside MessageDatabase:org.apache.activemq.store.kahadb.MessageDatabase.process(KahaRemoveMessageCommand, Location) - 10 secs elapsedorg.apache.activemq.store.kahadb.MessageDatabase.process(KahaAddMessageCommand, Location) - 8.5 secs elapsed.As both of these lock on indexLock.writeLock(), and both take place on the NIO transport threads, i think this accounts for at least some of the message throughput limits. As messages are added and removed from the index one by one, regardless of sync type settings, this adds a fair amount of overhead. While we're not synchronising on writes to disk, we are performing work on the NIO worker thread which can block on locks, and could account for the behaviour we've seen client side. To Reproduce:1. Install a broker and use the attached configuration.2. Use the 5.8.0 example ant script to consume from the queues, TopicQueueRouted.1 - 5. eg:   ant consumer -Durl=tcp://localhost:61616 -Dsubject=TopicQueueRouted.1 -Duser=admin -Dpassword=admin -Dmax=-13. Use the modified version of 5.8.0 example ant script (attached) to send messages to topics, TopicRouted.1 - 5, eg:   ant producer -Durl='tcp://localhost:61616?jms.useAsyncSend=true&wireFormat.tightEncodingEnabled=false&keepAlive=true&wireFormat.maxInactivityDuration=60000&socketBufferSize=32768' -Dsubject=TopicRouted.1 -Duser=admin -Dpassword=admin -Dmax=1 -Dtopic=true -DsleepTime=0 -Dmax=10000 -DmessageSize=5000This modified version of the script prints the number of messages per second and prints it to the console. Improvement Improve performance of composite topic fanout and persistent asyncSend We have publishers publishing to a topic which has 5 topic -> queue routings, and gets a max message rate attainable of ~833 messages/sec, with each message around 5k in size.To test this i set up a JMS config with topic queues:TopicTopicRouted.1...TopicRouted.11Each topic has an increasing number of routings to queues, and a client is set up to subscribe to all the queues.Rough message rates:routings messages/sec0 25001 14282 20003 14284 11115 833This occurs whether the broker config has producerFlowControl="false" set to true or false , and KahaDB disk synching is turned off. We also tried experimenting with concurrentStoreAndDispatch, but that didn't seem to help. LevelDB didn't give any notable performance improvement either.We also have asyncSend enabled on the producer, and have a requirement to use persistent messages. We have also experimented with sending messages in a transaction, but that hasn't really helped.It seems like producer throughput rate across all queue destinations, all connections and all publisher machines is limited by something on the broker, through a mechanism which is not producer flow control. I think the prime suspect is still contention on the index.We did some test with Yourkit profiler.Profiler was attached to broker at startup, allowed to run and then a topic publisher was started, routing to 5 queues. Profiler statistics were reset, the publisher allowed to run for 60 seconds, and then profiling snapshot was taken. During that time, ~9600 messages were logged as being sent for a rate of ~160/sec.This ties in roughly with the invocation counts recorded in the snapshot (i think) - ~43k calls. From what i can work out, in the snapshot (filtering everything but org.apache.activemq.store.kahadb), For the 60 second sample period, 24.8 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.removeAsyncMessage(ConnectionContext, MessageAck).18.3 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.asyncAddQueueMessage(ConnectionContext, Message, boolean).From these, a further large portion of the time is spent inside MessageDatabase:org.apache.activemq.store.kahadb.MessageDatabase.process(KahaRemoveMessageCommand, Location) - 10 secs elapsedorg.apache.activemq.store.kahadb.MessageDatabase.process(KahaAddMessageCommand, Location) - 8.5 secs elapsed.As both of these lock on indexLock.writeLock(), and both take place on the NIO transport threads, i think this accounts for at least some of the message throughput limits. As messages are added and removed from the index one by one, regardless of sync type settings, this adds a fair amount of overhead. While we're not synchronising on writes to disk, we are performing work on the NIO worker thread which can block on locks, and could account for the behaviour we've seen client side. To Reproduce:1. Install a broker and use the attached configuration.2. Use the 5.8.0 example ant script to consume from the queues, TopicQueueRouted.1 - 5. eg:   ant consumer -Durl=tcp://localhost:61616 -Dsubject=TopicQueueRouted.1 -Duser=admin -Dpassword=admin -Dmax=-13. Use the modified version of 5.8.0 example ant script (attached) to send messages to topics, TopicRouted.1 - 5, eg:   ant producer -Durl='tcp://localhost:61616?jms.useAsyncSend=true&wireFormat.tightEncodingEnabled=false&keepAlive=true&wireFormat.maxInactivityDuration=60000&socketBufferSize=32768' -Dsubject=TopicRouted.1 -Duser=admin -Dpassword=admin -Dmax=1 -Dtopic=true -DsleepTime=0 -Dmax=10000 -DmessageSize=5000This modified version of the script prints the number of messages per second and prints it to the console.
Update QPid client to v0.26 Update to latest release. Task Update QPid client to v0.26 Update to latest release.
Allow isSameRM override from broker identity to connection identity to avoid xa.join When two connection are involved in an xa transaction, 1pc is great and basing the xaresource identity on the broker identity makes sense. However in the cases that an joined xa association is not ended, which seems ok from an xa perspective, a joined association is left in error.To avoid this, we need a way to force 2pc across multiple connections to the same broker.A broker url param like jms.rmIdFromConnectionId=true will do the trick and ensure that the RM identity is tied to the connection and not the broker. Improvement Allow isSameRM override from broker identity to connection identity to avoid xa.join When two connection are involved in an xa transaction, 1pc is great and basing the xaresource identity on the broker identity makes sense. However in the cases that an joined xa association is not ended, which seems ok from an xa perspective, a joined association is left in error.To avoid this, we need a way to force 2pc across multiple connections to the same broker.A broker url param like jms.rmIdFromConnectionId=true will do the trick and ensure that the RM identity is tied to the connection and not the broker.
Temp Queue gets deleted on close of wrong connection My scenario is this:connection1:create temp queue tq1send msg to qeue1 with replyTo tq1wait for reply on tq1connection2:receive message on queue1send to replyTo address which is tq1In some cases the temp queue gets deleted in the close method of connection2.The scenario is kind of an edge case as it only happens if I use a PooledConnectionFactory and only if I before my scenario above open a connection and session and close the connection before the session.So strictly speaking my code has an error. I think the problem is in the PooledConnection factory. It seems to reuse a connection or session in the wrong way. I will attach a test case Improvement Temp Queue gets deleted on close of wrong connection My scenario is this:connection1:create temp queue tq1send msg to qeue1 with replyTo tq1wait for reply on tq1connection2:receive message on queue1send to replyTo address which is tq1In some cases the temp queue gets deleted in the close method of connection2.The scenario is kind of an edge case as it only happens if I use a PooledConnectionFactory and only if I before my scenario above open a connection and session and close the connection before the session.So strictly speaking my code has an error. I think the problem is in the PooledConnection factory. It seems to reuse a connection or session in the wrong way. I will attach a test case
Statistics plugin doesn't fill in Message timestamp or priority  StatisticsBrokerPlugin should fill in Message timestamp and priority  Improvement Statistics plugin doesn't fill in Message timestamp or priority  StatisticsBrokerPlugin should fill in Message timestamp and priority 
Upgrade Proton to v0.6 Upgrade to latest release of proton.   Improvement Upgrade Proton to v0.6 Upgrade to latest release of proton.  
Disable jar indexing ActiveMQ's pom.xml uses maven-jar-plugin with <index>true</index>, which creates META-INF/INDEX.LIST file in the JAR files. This is useless when done only for a single jar file and might in some cases even be harmful.I propose to disable the generation INDEX.LIST files.For a full discussion of the drawbacks, see the corresponding bug track at Tomcat's Bugzilla: https://issues.apache.org/bugzilla/show_bug.cgi?id=49236 Improvement Disable jar indexing ActiveMQ's pom.xml uses maven-jar-plugin with <index>true</index>, which creates META-INF/INDEX.LIST file in the JAR files. This is useless when done only for a single jar file and might in some cases even be harmful.I propose to disable the generation INDEX.LIST files.For a full discussion of the drawbacks, see the corresponding bug track at Tomcat's Bugzilla: https://issues.apache.org/bugzilla/show_bug.cgi?id=49236
Better protect worker thread in TcpTransportServer that handles socket accepts The worker thread in the TcpTransportServer that handles socket accepts from a queue can die if the handle method allows a throwable to escape.  We should catch these and log them as warn if the transport isn't stopping as this is unexpected and we don't want this thread to die and stop servicing incoming connections.   Improvement Better protect worker thread in TcpTransportServer that handles socket accepts The worker thread in the TcpTransportServer that handles socket accepts from a queue can die if the handle method allows a throwable to escape.  We should catch these and log them as warn if the transport isn't stopping as this is unexpected and we don't want this thread to die and stop servicing incoming connections.  
Update the JobSchedulerStoreImpl in KahaDB to use LockableServiceSupport Implement LockableServiceSupport in the KahaDB Scheduler store so that it can be configured to use the same retry values as the KahaDB locker.   Improvement Update the JobSchedulerStoreImpl in KahaDB to use LockableServiceSupport Implement LockableServiceSupport in the KahaDB Scheduler store so that it can be configured to use the same retry values as the KahaDB locker.  
Add new mode to JMS Pool that allows for not caching producers The current JMS Pool creates a single anonymous producer instance for all requests to create a Producer.  In some cases a user might want to have a separate producer instance created for each requestor.  We will add a new option on the PooledConnectionFactory to have all PooledSessions create separate MessageProducers, TopicPublishers and QueueSenders for each create call.   New Feature Add new mode to JMS Pool that allows for not caching producers The current JMS Pool creates a single anonymous producer instance for all requests to create a Producer.  In some cases a user might want to have a separate producer instance created for each requestor.  We will add a new option on the PooledConnectionFactory to have all PooledSessions create separate MessageProducers, TopicPublishers and QueueSenders for each create call.  
Reduce the reliance on fsync when writing to disk Moving AMQ from  RHEL 4 to RHEL 6 affects performance of kahadb writes as seen from the DiskBenchmark Improvement Reduce the reliance on fsync when writing to disk Moving AMQ from  RHEL 4 to RHEL 6 affects performance of kahadb writes as seen from the DiskBenchmark
include activemq-jms-pool in activemq-all on upgrade from 5.8 to 5.9{code}<bean id="con-factory"class="org.apache.activemq.spring.ActiveMQConnectionFactory">        <property name="brokerURL" value="tcp://xxxxx:xxx" /></bean><bean id="activemq-for-consumer"class="org.apache.activemq.pool.PooledConnectionFactoryBean"                destroy-method="stop">        <property name="connectionFactory" ref="con-factory"></property></bean>{code}When I ran the old project,I got the exception:java.lang.ClassNotFoundException:org.apache.activemq.jms.pool.PooledConnectionFactoryI checked the  activemq-all-5.9.0.jarand didn't see the package oforg.apache.activemq.jms in it. New Feature include activemq-jms-pool in activemq-all on upgrade from 5.8 to 5.9{code}<bean id="con-factory"class="org.apache.activemq.spring.ActiveMQConnectionFactory">        <property name="brokerURL" value="tcp://xxxxx:xxx" /></bean><bean id="activemq-for-consumer"class="org.apache.activemq.pool.PooledConnectionFactoryBean"                destroy-method="stop">        <property name="connectionFactory" ref="con-factory"></property></bean>{code}When I ran the old project,I got the exception:java.lang.ClassNotFoundException:org.apache.activemq.jms.pool.PooledConnectionFactoryI checked the  activemq-all-5.9.0.jarand didn't see the package oforg.apache.activemq.jms in it.
Is it hawtio ui bug? when I want to give a health test through jtester 2.9,exception occurs as follows:2013/12/03 10:14:19 ERROR - jmeter.monitor.parser.ParserImpl: Failed to parse the bytes org.xml.sax.SAXParseException; lineNumber: 50; columnNumber: 3; The element type "link" must be terminated by the matching end-tag "</link>".may be new hawtio console html can not vell-defined ? Improvement Is it hawtio ui bug? when I want to give a health test through jtester 2.9,exception occurs as follows:2013/12/03 10:14:19 ERROR - jmeter.monitor.parser.ParserImpl: Failed to parse the bytes org.xml.sax.SAXParseException; lineNumber: 50; columnNumber: 3; The element type "link" must be terminated by the matching end-tag "</link>".may be new hawtio console html can not vell-defined ?
The examples/swissarmy should setup slf4j/log4j when running So when you run this example the slfj4 doesnt complain and you can configure log4j.properties to have logging levels you wantdavsclaus:/opt/apache-activemq-5.10-SNAPSHOT/examples/openwire/swissarmy$ ant consumerBuildfile: /opt/apache-activemq-5.10-SNAPSHOT/examples/openwire/swissarmy/build.xmlinit:compile:consumer:     [echo] Running consumer against server at $url = tcp://localhost:61616 for subject $subject = TEST.FOO     [java] Connecting to URL: tcp://localhost:61616 (:)     [java] Consuming queue: TEST.FOO     [java] Using a non-durable subscription     [java] Running 1 parallel threads     [java] log4j:WARN No appenders could be found for logger (org.apache.activemq.transport.WireFormatNegotiator).     [java] log4j:WARN Please initialize the log4j system properly.     [java] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.     [java] [Thread-1] We are about to wait until we consume: 2000 message(s) then we will shutdown Improvement The examples/swissarmy should setup slf4j/log4j when running So when you run this example the slfj4 doesnt complain and you can configure log4j.properties to have logging levels you wantdavsclaus:/opt/apache-activemq-5.10-SNAPSHOT/examples/openwire/swissarmy$ ant consumerBuildfile: /opt/apache-activemq-5.10-SNAPSHOT/examples/openwire/swissarmy/build.xmlinit:compile:consumer:     [echo] Running consumer against server at $url = tcp://localhost:61616 for subject $subject = TEST.FOO     [java] Connecting to URL: tcp://localhost:61616 (:)     [java] Consuming queue: TEST.FOO     [java] Using a non-durable subscription     [java] Running 1 parallel threads     [java] log4j:WARN No appenders could be found for logger (org.apache.activemq.transport.WireFormatNegotiator).     [java] log4j:WARN Please initialize the log4j system properly.     [java] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.     [java] [Thread-1] We are about to wait until we consume: 2000 message(s) then we will shutdown
Typos in MessageStore Thes typos in the "addSubsciption" method and the "memoeyUSage" parameter need to be fixed. Task Typos in MessageStore Thes typos in the "addSubsciption" method and the "memoeyUSage" parameter need to be fixed.
Fusesource release repo not needed The proton dependencies are at central.  Thus, the fusesource release repo in the top level pom is no longer needed and can slow down the build (as well as cause extra required configuration if behind a firewall).{code}diff --git a/pom.xml b/pom.xmlindex eb05b07..ffd0bef 100755--- a/pom.xml+++ b/pom.xml@@ -1590,13 +1590,6 @@   </profiles>    <repositories>-    <!-- for the proton dependency -->-    <repository>-      <id>com.fusesource.m2</id>-      <url>http://repo.fusesource.com/nexus/content/groups/public/</url>-      <releases><enabled>true</enabled></releases>-      <snapshots><enabled>false</enabled></snapshots>-    </repository>     <!-- for the paho dependency -->     <repository>       <id>eclipse.m2</id>{code} Task Fusesource release repo not needed The proton dependencies are at central.  Thus, the fusesource release repo in the top level pom is no longer needed and can slow down the build (as well as cause extra required configuration if behind a firewall).{code}diff --git a/pom.xml b/pom.xmlindex eb05b07..ffd0bef 100755--- a/pom.xml+++ b/pom.xml@@ -1590,13 +1590,6 @@   </profiles>    <repositories>-    <!-- for the proton dependency -->-    <repository>-      <id>com.fusesource.m2</id>-      <url>http://repo.fusesource.com/nexus/content/groups/public/</url>-      <releases><enabled>true</enabled></releases>-      <snapshots><enabled>false</enabled></snapshots>-    </repository>     <!-- for the paho dependency -->     <repository>       <id>eclipse.m2</id>{code}
Align xbean and upgrade to 3.15 We use xbean 3.14 and 3.12 for the maven plugin. We should align these versions and use latest 3.15. Improvement Align xbean and upgrade to 3.15 We use xbean 3.14 and 3.12 for the maven plugin. We should align these versions and use latest 3.15.
Scala source file forgotten in /src/main/java Hi,I noticed a forgotten scala source file in src/main/java. It probably belongs to src/main/javait's the file activemq-leveldb-store/src/main/java/org/apache/activemq/leveldb/replicated/ReplicatedLevelDBStoreTrait.scala in current trunk Task Scala source file forgotten in /src/main/java Hi,I noticed a forgotten scala source file in src/main/java. It probably belongs to src/main/javait's the file activemq-leveldb-store/src/main/java/org/apache/activemq/leveldb/replicated/ReplicatedLevelDBStoreTrait.scala in current trunk
Change MIME type for XML in the REST API According to RFC 3023, "if an XML document is readable by casual users, text/xml is preferable to application/xml. MIME user agents (and web user agents) that do not have explicit support for text/xml will treat it as text/plain, for example, by displaying the XML MIME entity as plain text. Application/xml is preferable when the XML MIME entity is unreadable by casual users".Several other projects supported by Red Hat (Drools, Infinispan, OpenShift) already comply with this RFC. Improvement Change MIME type for XML in the REST API According to RFC 3023, "if an XML document is readable by casual users, text/xml is preferable to application/xml. MIME user agents (and web user agents) that do not have explicit support for text/xml will treat it as text/plain, for example, by displaying the XML MIME entity as plain text. Application/xml is preferable when the XML MIME entity is unreadable by casual users".Several other projects supported by Red Hat (Drools, Infinispan, OpenShift) already comply with this RFC.
Show clientId view of duplex network connection Mbeans from http://mail-archives.apache.org/mod_mbox/activemq-users/201311.mbox/%3C35EC11A1-A490-4942-86D8-1238EA085DC3@schubergphilis.com%3Ewith duplex=true we only have the clientip view, the connectionInfo bypasses the initiating connection b/c that is now bridging to the local broker.I think we can fix this. Improvement Show clientId view of duplex network connection Mbeans from http://mail-archives.apache.org/mod_mbox/activemq-users/201311.mbox/%3C35EC11A1-A490-4942-86D8-1238EA085DC3@schubergphilis.com%3Ewith duplex=true we only have the clientip view, the connectionInfo bypasses the initiating connection b/c that is now bridging to the local broker.I think we can fix this.
